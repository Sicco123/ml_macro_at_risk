I want you to create a py file. That will be able to run my code in parallel. You are an expert on deep learning and parallel programming. 

Make code for me that:
- reads a config
- checks which model is going to be run ar-qr, lqr, nn (each model will have its own parameters) and I am going to use different nn versions. All need to be stored separately
- Check running the model with a number of training iterations equal to 1 to estimate the memory usage. Based on that I want you check in the config how much memory is available. Also check how many cores are availalbe. Run the models in parallel without going beyond the memory or core limits. 
- Then I want an rw estimation. Read the window size. Then we use the data forecast_start_date - rw_size : forecast_start_date. We get the forecast based on the horizon index. (there are going to be more, but always one model is trained for each horizon). I will do quantile forecasting. For each quantile I will run a separate model. 
- Then I want you to store the results. pkl each of the models and store it for each of the new window iterations. So every window step one new model. If the pkl was already available for that time period, load it instead of training the model if the configs allows for reloading if not train again. 
- I want a parquet files for each quantile and horizon that contain the rolling window forecasts. for all countries. File needs to be structured like cols: TIME COUNTRY TRUE_DATA FORECAST. 
- Then it is important that you create a file that keeps track for each model, quantile horizon combination whether the model is not trained yet, is training or is done training
- If one of the models fails or one of the processes. I want you continue the other processes. But save the error and also clearly indicate in the process file which time window still needs to be done. 
- When running the code again and the config says to not retrain. Check in the progress file which time windows still need to be trained and only train those. 
- I have programmed all the models already. An example of how to use those and how data is run can be seen in the run_analysis.py 
- A start for the yaml file is also given


Am I missing anything? Can you ask me questions first to avoid any unclarities. Later I will tell you to produce the code and config

1) Runtime + resources
	1.	Hardware: CPU-only, GPU, or mixed? If GPU: how many GPUs, what framework (PyTorch?), and should we cap GPU memory per process?
Default: CPU-only, PyTorch optional for NN on CPU.
	2.	Parallel backend: Python multiprocessing / concurrent.futures, joblib, or ray?
Default: concurrent.futures.ProcessPoolExecutor with per-task resource checks.
	3.	Resource detection: OK to use psutil to read total/free RAM and os.cpu_count() for cores?
Default: yes. RAM threshold = available * safety_factor (e.g., 0.8).

2) Config shape (single YAML/TOML/JSON?)
	4.	Format: YAML, TOML, or JSON?
Default: YAML.
	5.	Key sections:
	•	data: (paths, time column name, country column, target column).
	•	rolling_window: (start_date, end_date, window_size, step, horizons, quantiles).
	•	models: list of model specs with type ∈ {ar-qr, lqr, nn}, per-model params (including “nn_version”).
	•	runtime: (max_cores, max_ram_gb, safety_factor, allow_reload, retries).
	•	io: (root output dir, pkl dir, parquet dir, progress file path, logs dir).
OK? Anything to add?
	6.	Reload behavior: flag names?
Default: allow_reload: true, retrain_if_exists: false.

3) Data + rolling windows
	7.	Input data format & path: one big parquet/csv for all countries, or per-country files?
Default: one parquet, columns: TIME, COUNTRY, TARGET. (You said TRUE_DATA in outputs; I’ll treat TARGET in input as the ground truth.)
	8.	Time column type: daily/weekly/monthly? Any timezone?
Default: daily, naive UTC timestamps.
	9.	Windowing:
	•	forecast_start_date source—global or per-country?
	•	window_size units (days/periods)?
	•	step size?
Default: global forecast_start_date; window_size and step are integers in periods; we align per-country by intersection of available history.
	10.	Horizon index: are horizons measured in periods ahead (e.g., 1..H)?
Default: yes, integer steps ahead relative to last time in window.

4) Models & training
	11.	Model definitions:

	•	ar-qr: AutoRegressive Quantile Regression. Should I use statsmodels’ QuantReg with lagged features?
	•	lqr: Linear Quantile Regression with arbitrary features (lags, maybe calendar)?
	•	nn: PyTorch MLP/RNN/TemporalConv? Please specify architectures (“nn_version” variants).
Default:
	•	ar-qr: create lag features up to p, fit QuantReg per quantile.
	•	lqr: same features, but permit extra regressors (if provided).
	•	nn: simple PyTorch MLP with lags; nn_version switches depth/width/dropout.

	12.	Quantile strategy: separate model per quantile (as you requested). OK to share features/normalization?
Default: Yes; scaler fit inside each window (saved with model).
	13.	Memory estimation: run 1 training iteration (or 1 epoch for NN) to estimate peak RAM (and GPU if relevant), then schedule tasks so sum(estimated_memory) ≤ available & processes ≤ core cap.
Default: use a dry-run with same batch shapes, use tracemalloc + RSS sampling for RAM; for GPU (if used) query torch.cuda.max_memory_allocated().
	14.	Cores per model: any model using multithreading (e.g., MKL) should be pinned to 1 thread to respect core limits. OK to set OMP_NUM_THREADS=1, MKL_NUM_THREADS=1 per worker?
Default: yes.

5) Caching, outputs, progress
	15.	PKL format: joblib or pickle? For PyTorch, save both .pkl metadata and .pt weights?
Default: joblib for classical models, .pt for NN plus JSON metadata.
	16.	Directory structure (proposed):


17) Progress file schema (proposed):
Columns: MODEL, NN_VERSION, QUANTILE, HORIZON, COUNTRY, WINDOW_START, WINDOW_END, STATUS ∈ {pending, training, done, failed}, LAST_UPDATE, ERROR_MSG (nullable), MODEL_PATH, FORECAST_ROWS. Good?
18) Idempotency: on re-run with allow_reload: true, we skip rows where STATUS == done and pkl exists; if retrain_if_exists: false we respect cache, else force retrain. Good?

6) Forecast assembly
	19.	Output parquet schema: per your spec: TIME, COUNTRY, TRUE_DATA, FORECAST.

	•	For horizons > 1, forecasts align to TIME = last_window_time + horizon. OK?
	•	Do you also want the MODEL, QUANTILE, HORIZON stored as columns? (Often helpful.)
Default: include extra columns (MODEL, NN_VERSION, QUANTILE, HORIZON, WINDOW_START, WINDOW_END) for traceability.

	20.	All countries: train per-country models (i.e., separate model per country) or global model with COUNTRY as a feature?
Default: per-country models.

7) Failure handling & retries
	21.	Retries: number of automatic retries on failure (e.g., 2)?
Default: 1 retry; then mark failed and log exception trace.
	22.	Continue on failure: proceed with other tasks, leave failed windows pending for future reruns. OK?
Default: yes.

8) Reproducibility & logging
	23.	Random seeds: fixed global seed (e.g., 42)?
Default: yes; set for numpy, random, and torch if used.
	24.	Logging level: INFO/DEBUG? Single rotating log file or per-run file?
Default: INFO, per-run timestamped log plus console.

9) Packaging & entrypoint
	25.	Single runner.py that: loads config → builds plan → estimates memory → schedules tasks → writes progress/forecasts. CLI args: --config path.yaml, --dry-run, --max-workers override, --only model=lqr,nn, etc. OK?
	26.	Dependencies: pandas, pyarrow, psutil, joblib, patsy/statsmodels (for QR), torch (optional), pyyaml, tqdm. Any constraints?
Default: those.

10) Nice-to-haves (optional—say yes/no)
	27.	File locking to avoid race conditions on shared parquet/progress (via fasteners or filelock).
	28.	Progress dashboard: tiny progress.html updated live (no server) aggregating the parquet status.
	29.	Early stopping for NN and standardization (z-score) per window.
	30.	Feature engineering hooks: allow user-defined Python callback to add features for lqr/nn.

If you confirm these (or tweak them), I’ll generate:
	•	runner.py (single file, production-ready)
	•	config.yaml template with examples for ar-qr, lqr, and multiple nn_versions
	•	Minimal model implementations and utilities (inside runner.py) so it’s truly one file
	•	Clear folder structure and naming
	•	Robust progress/error handling and resumption logic

Anything else you want baked in (e.g., SLURM support, GPU pinning, holiday features, cross-val inside window)?


# an inspiration but the problem is here that the models are coded again. I do not want that. I have coded them already and they are in run_analysis.py. Also let me do the analysis for other target variables. 

#!/usr/bin/env python3
"""
Quantile Forecast Runner (CPU-only, SLURM-aware)

Features
- Reads YAML config (template embedded below; write with --write-config)
- Supports model types: ar-qr (autoregressive quantile regression), lqr (linear quantile regression + optional RFF), ensemble_nn (MLP) — each per quantile & horizon
- Rolling-window training & forecasting over per-country datasets (directory of parquet/csv files)
- Dry-run memory probe (1 training iteration) to estimate per-task RAM, then parallel scheduling under CPU/RAM caps
- Robust resume: caches models per (country, model, quantile, horizon, window); skips when allowed
- Progress tracking parquet+json (pending/training/done/failed) with static HTML dashboard
- File locking for models, parquet consolidation, and progress updates
- SLURM aware (respects SLURM_CPUS*, SLURM_MEM*; also cgroup memory)
- Safe MKL/OMP pinning to 1 thread per worker

Dependencies
  pip install pandas pyarrow pyyaml psutil filelock joblib tqdm statsmodels torch

Usage
  python quant_forecast_runner.py --write-config config.yaml
  python quant_forecast_runner.py --config config.yaml

Notes
- Data directory (config.data.path) should contain per-country parquet/csv files. If a COUNTRY column is missing
  it's inferred from filename stem. TIME must be parseable to datetime.
- Output parquet per (quantile, horizon) contain columns: TIME, COUNTRY, TRUE_DATA, FORECAST (plus metadata columns).
- ensemble_nn requires PyTorch; runs on CPU even if CUDA exists (config enforces CPU-only here).

"""
from __future__ import annotations

import os
import sys
import gc
import json
import math
import time
import glob
import uuid
import yaml
import queue
import errno
import psutil
import random
import logging
import inspect
import threading
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any

import numpy as np
import pandas as pd
from filelock import FileLock
from tqdm import tqdm

# pyarrow for parquet
import pyarrow as pa
import pyarrow.parquet as pq

# Optional heavy deps
try:
    import statsmodels.api as sm
    from statsmodels.regression.quantile_regression import QuantReg
except Exception:
    sm = None
    QuantReg = None

try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
except Exception:
    torch = None

from concurrent.futures import ProcessPoolExecutor, as_completed

# ----------------------------- Default Config Template -----------------------------
DEFAULT_CONFIG_YAML = """
seed: 123

runtime:
  allow_reload: true               # if true, load existing models/forecasts instead of retraining
  retrain_if_exists: false         # if true, force retrain even if model artifact exists
  max_cores: auto                  # integer or 'auto'
  safety_ram_fraction: 0.8         # use at most this fraction of detected available RAM
  max_ram_gb: auto                 # hard cap; number or 'auto'
  mem_probe_fudge_mb: 200          # add this to each task's measured memory (to be conservative)
  retries: 1                       # automatic retries per failed task
  progress_refresh_sec: 5          # how often to rewrite dashboard (seconds)

io:
  data_path: processed_per_country # directory with per-country parquet/csv
  output_root: outputs             # root folder
  models_dir: models
  forecasts_dir: forecasts
  progress_dir: progress
  logs_dir: logs
  dashboard_html: progress/dashboard.html
  progress_parquet: progress/progress.parquet
  errors_parquet: progress/errors.parquet

# ---------------- Data & Preprocessing ----------------

data:
  target: prc_hicp_manr_CP00
  required_columns: []             # extra columns required; if missing, raise
  lags: []                         # if empty, ar-qr/lqr will default to 1..12 lags of target
  horizons: [1]
  quantiles: [0.1]
  missing: forward_fill_then_mean  # or interpolate_linear | drop
  scale: per_country               # or global | none
  scale_target: false
  frequency: infer                 # infer and validate regular spacing
  trimming: true                   # drop leading/trailing NaNs created by lags
  country_dummies: false           # add COUNTRY one-hots (lqr/nn)

splits:
  validation_size: 0.3
  train_start: "1997-01-01"
  test_cutoff: "2017-12-01"        # last date in training; windows roll from here forward
  min_train_points: 12

# Rolling window settings
rolling_window:
  size: 24                         # number of periods in each window (e.g., months)
  step: 1                          # advance per window
  start: auto                      # 'auto' => first forecast_start at test_cutoff
  end: auto                        # 'auto' => last possible date given data & horizon

# ---------------- Models ----------------
# Enable/disable models by presence and `enabled: true`

ar_qr:
  enabled: true
  p: 12                            # number of AR lags; ignored if data.lags provided
  add_drift: true

lqr:
  enabled: true
  alphas: [7.5, 10, 15, 20, 25, 30, 35]  # Huberized loss strength (if solver supports)
  solver: huberized
  rff_features: 0                   # 0 disables Random Fourier Features
  rolling: false

ensemble_nn:
  enabled: true
  units_per_layer: [32,32,32,32,32]
  activation: relu
  optimizer: adam
  learning_rate: 5.0e-5
  epochs: 1500
  batch_size: 32
  patience: 50
  parallel_models: 10
  per_country: false
  device: cpu                      # forced to cpu in this runner
  country_dummies: false
  l2_penalty: 0.0001

output:
  save_predictions: true
  save_factors: false
  save_models: true
"""

# ----------------------------- Logging Setup -----------------------------

def setup_logging(log_dir: Path) -> None:
    log_dir.mkdir(parents=True, exist_ok=True)
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_path = log_dir / f"run_{ts}.log"
    handlers = [logging.FileHandler(log_path), logging.StreamHandler(sys.stdout)]
    logging.basicConfig(
        level=logging.INFO,
        format="[%(asctime)s][%(levelname)s] %(message)s",
        handlers=handlers,
    )
    logging.info(f"Logging to {log_path}")

# ----------------------------- Utils -----------------------------

def set_global_seeds(seed: int) -> None:
    random.seed(seed)
    np.random.seed(seed)
    if torch is not None:
        torch.manual_seed(seed)
        torch.use_deterministic_algorithms(False)


def read_yaml(path: Path) -> Dict[str, Any]:
    with open(path, "r") as f:
        return yaml.safe_load(f)


def write_text(path: Path, text: str) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, "w") as f:
        f.write(text)


def detect_cpu_cores() -> int:
    # Respect SLURM if present
    env_vals = [
        os.environ.get("SLURM_CPUS_PER_TASK"),
        os.environ.get("SLURM_CPUS_ON_NODE"),
        os.environ.get("SLURM_NTASKS"),
        os.environ.get("SLURM_TASKS_PER_NODE"),
    ]
    env_cores = [int(v) for v in env_vals if v and v.isdigit() and int(v) > 0]
    try:
        aff = len(os.sched_getaffinity(0))
    except Exception:
        aff = os.cpu_count() or 1
    detected = min([c for c in [aff, *(env_cores or [aff])] if c and c > 0])
    return max(1, detected)


def _read_cgroup_mem_limit_bytes() -> Optional[int]:
    # Try cgroup v2 then v1
    candidates = [
        "/sys/fs/cgroup/memory.max",
        "/sys/fs/cgroup/memory/memory.limit_in_bytes",
    ]
    for p in candidates:
        try:
            if os.path.exists(p):
                val = open(p).read().strip()
                if val.isdigit():
                    b = int(val)
                    if b > 0 and b < 1 << 60:  # filter 'max' or huge
                        return b
        except Exception:
            pass
    return None


def detect_available_ram_bytes() -> int:
    vm = psutil.virtual_memory()
    avail = vm.available
    # SLURM hints
    for k in ["SLURM_MEM_PER_NODE", "SLURM_MEM_PER_CPU"]:
        v = os.environ.get(k)
        if v and v.isdigit():
            mb = int(v)
            if k == "SLURM_MEM_PER_CPU":
                mb *= detect_cpu_cores()
            avail = min(avail, mb * 1024 * 1024)
    cgroup = _read_cgroup_mem_limit_bytes()
    if cgroup:
        avail = min(avail, cgroup)
    return max(256 * 1024 * 1024, int(avail))


def bytes_to_gb(b: int) -> float:
    return b / (1024 ** 3)


def safe_mkdirs(path: Path) -> None:
    path.mkdir(parents=True, exist_ok=True)

# ----------------------------- File Lock Helpers -----------------------------

def lock_path(path: Path) -> Path:
    return path.with_suffix(path.suffix + ".lock")

# ----------------------------- Data Loading & Preprocessing -----------------------------

REQUIRED_COLUMNS_BASE = ["TIME"]


def load_country_files(data_dir: Path) -> Dict[str, Path]:
    files = []
    for ext in ("*.parquet", "*.pq", "*.feather", "*.csv"):  # support csv too
        files.extend(data_dir.glob(ext))
    mapping = {}
    for f in files:
        country = f.stem.upper().split("__")[0]
        mapping[country] = f
    if not mapping:
        raise FileNotFoundError(f"No data files found in {data_dir}")
    return mapping


def read_df(path: Path) -> pd.DataFrame:
    if path.suffix.lower() in [".parquet", ".pq", ".feather"]:
        return pd.read_parquet(path)
    elif path.suffix.lower() == ".csv":
        return pd.read_csv(path)
    else:
        raise ValueError(f"Unsupported file type: {path.suffix}")


def ensure_time_index(df: pd.DataFrame, time_col: str = "TIME") -> pd.DataFrame:
    if time_col not in df.columns:
        raise ValueError(f"Missing TIME column '{time_col}'")
    df = df.copy()
    df[time_col] = pd.to_datetime(df[time_col])
    df.sort_values(time_col, inplace=True)
    df.reset_index(drop=True, inplace=True)
    return df


def handle_missing(df: pd.DataFrame, how: str) -> pd.DataFrame:
    df = df.copy()
    if how == "forward_fill_then_mean":
        df = df.ffill()
        df = df.fillna(df.mean(numeric_only=True))
    elif how == "interpolate_linear":
        df = df.interpolate(method="linear")
    elif how == "drop":
        df = df.dropna()
    else:
        raise ValueError(f"Unknown missing method: {how}")
    return df


def add_lags(df: pd.DataFrame, col: str, lags: List[int]) -> pd.DataFrame:
    out = df.copy()
    for L in lags:
        out[f"{col}_lag{L}"] = out[col].shift(L)
    return out


def scale_df(df: pd.DataFrame, cols: List[str], with_target: bool, global_stats: Optional[Tuple[np.ndarray, np.ndarray]] = None) -> Tuple[pd.DataFrame, Tuple[np.ndarray, np.ndarray]]:
    out = df.copy()
    X = out[cols].values.astype(float)
    if global_stats is None:
        mu = np.nanmean(X, axis=0)
        sigma = np.nanstd(X, axis=0) + 1e-8
    else:
        mu, sigma = global_stats
    Xn = (X - mu) / sigma
    out[cols] = Xn
    return out, (mu, sigma)


# ----------------------------- Rolling Window Logic -----------------------------

def generate_windows(dates: pd.Series, size: int, step: int, start_date: pd.Timestamp, end_date: pd.Timestamp, horizon: int) -> List[Tuple[pd.Timestamp, pd.Timestamp, pd.Timestamp]]:
    """Return list of (win_start, win_end, forecast_time) where forecast_time = win_end + horizon_periods.
    We align windows so that win_end == forecast_start_date - 0 periods (i.e., inclusive),
    and window covers [win_end - size + 1, win_end].
    """
    date_index = pd.Index(dates)
    out = []
    # Allowed forecast_start_date range
    fs_min = max(start_date, dates.iloc[0])
    fs_max = min(end_date, dates.iloc[-1])
    # Move forecast_start from fs_min to fs_max stepping by calendar steps present in dates
    all_times = date_index.values
    # Build mapping pos -> time
    # For simplicity we step by index positions
    for i in range(len(date_index)):
        fs = date_index[i]
        if fs < fs_min or fs > fs_max:
            continue
        win_end = fs
        # Find position of fs in index
        # window start index
        j = i - (size - 1)
        if j < 0:
            continue
        win_start = date_index[j]
        # forecast emission time for horizon h is date at position i + h
        k = i + horizon
        if k >= len(date_index):
            break
        forecast_time = date_index[k]
        out.append((win_start, win_end, forecast_time))
        # advance by step in index space
        i += step - 1
    return out

# ----------------------------- Models -----------------------------

class BaseQuantModel:
    def __init__(self, quantile: float):
        self.quantile = quantile

    def fit(self, df: pd.DataFrame, feature_cols: List[str], target_col: str, max_iter: int = None):
        raise NotImplementedError

    def predict(self, df: pd.DataFrame, feature_cols: List[str]) -> np.ndarray:
        raise NotImplementedError

    def save(self, path: Path) -> None:
        raise NotImplementedError

    @classmethod
    def load(cls, path: Path) -> "BaseQuantModel":
        raise NotImplementedError


class LQRModel(BaseQuantModel):
    """Linear Quantile Regression via statsmodels QuantReg.
    Supports optional Random Fourier Features (RFF) expansion.
    """
    def __init__(self, quantile: float, rff_features: int = 0, random_state: int = 123):
        super().__init__(quantile)
        self.rff_features = int(rff_features or 0)
        self.random_state = random_state
        self.beta_: Optional[np.ndarray] = None
        self.feature_cols: List[str] = []
        self.rff_W_: Optional[np.ndarray] = None
        self.rff_b_: Optional[np.ndarray] = None
        self._scaler_: Optional[Tuple[np.ndarray, np.ndarray]] = None

        if QuantReg is None:
            raise ImportError("statsmodels is required for lqr/ar-qr models")

    def _rff(self, X: np.ndarray) -> np.ndarray:
        if self.rff_features <= 0:
            return X
        rng = np.random.RandomState(self.random_state)
        d = X.shape[1]
        m = self.rff_features
        # Heuristic gamma = 1/d; draw W ~ N(0, 2*gamma)
        gamma = 1.0 / max(1, d)
        if self.rff_W_ is None:
            self.rff_W_ = rng.normal(0, np.sqrt(2 * gamma), size=(d, m))
            self.rff_b_ = rng.uniform(0, 2 * np.pi, size=(m,))
        Z = np.cos(X @ self.rff_W_ + self.rff_b_)
        return Z

    def fit(self, df: pd.DataFrame, feature_cols: List[str], target_col: str, max_iter: int = None):
        self.feature_cols = list(feature_cols)
        X = df[self.feature_cols].astype(float).values
        y = df[target_col].astype(float).values
        X = self._rff(X)
        X = sm.add_constant(X, has_constant='add')
        model = QuantReg(y, X)
        res = model.fit(q=self.quantile, max_iter=max_iter)
        self.beta_ = res.params.astype(float)
        return self

    def predict(self, df: pd.DataFrame, feature_cols: List[str]) -> np.ndarray:
        X = df[feature_cols].astype(float).values
        X = self._rff(X)
        X = sm.add_constant(X, has_constant='add')
        return X @ self.beta_

    def save(self, path: Path) -> None:
        path.parent.mkdir(parents=True, exist_ok=True)
        meta = {
            "quantile": self.quantile,
            "rff_features": self.rff_features,
            "feature_cols": self.feature_cols,
            "beta": self.beta_.tolist() if self.beta_ is not None else None,
        }
        with open(path, "w") as f:
            json.dump(meta, f)

    @classmethod
    def load(cls, path: Path) -> "LQRModel":
        meta = json.load(open(path))
        m = cls(quantile=meta["quantile"], rff_features=meta.get("rff_features", 0))
        m.feature_cols = meta["feature_cols"]
        m.beta_ = np.array(meta["beta"], dtype=float)
        return m


class ARQRModel(LQRModel):
    """AR-QR is LQR with only lag features and optional drift."""
    pass


class MLP(nn.Module):
    def __init__(self, in_dim: int, units: List[int], activation: str = "relu"):
        super().__init__()
        act = {
            "relu": nn.ReLU,
            "tanh": nn.Tanh,
            "gelu": nn.GELU,
            "leaky_relu": nn.LeakyReLU,
        }.get(activation, nn.ReLU)
        layers: List[nn.Module] = []
        prev = in_dim
        for u in units:
            layers += [nn.Linear(prev, u), act()]
            prev = u
        layers += [nn.Linear(prev, 1)]
        self.net = nn.Sequential(*layers)

    def forward(self, x):
        return self.net(x)


class NNQuantileModel(BaseQuantModel):
    def __init__(self, quantile: float, units: List[int], lr: float, epochs: int, batch_size: int, patience: int, l2_penalty: float):
        super().__init__(quantile)
        if torch is None:
            raise ImportError("PyTorch is required for ensemble_nn model")
        self.units = units
        self.lr = lr
        self.epochs = epochs
        self.batch_size = batch_size
        self.patience = patience
        self.l2_penalty = l2_penalty
        self.model: Optional[MLP] = None
        self.in_dim = 0

    def pinball(self, y_pred, y_true):
        q = self.quantile
        e = y_true - y_pred
        return torch.maximum(q * e, (q - 1) * e).mean()

    def fit(self, df: pd.DataFrame, feature_cols: List[str], target_col: str, max_iter: int = None):
        X = df[feature_cols].astype(float).values
        y = df[target_col].astype(float).values.reshape(-1, 1)
        self.in_dim = X.shape[1]
        model = MLP(self.in_dim, self.units)
        opt = optim.Adam(model.parameters(), lr=self.lr, weight_decay=self.l2_penalty)
        X_t = torch.from_numpy(X).float()
        y_t = torch.from_numpy(y).float()
        best_loss = float('inf')
        best_state = None
        no_improve = 0
        ds = torch.utils.data.TensorDataset(X_t, y_t)
        loader = torch.utils.data.DataLoader(ds, batch_size=self.batch_size, shuffle=True)
        epochs = 1 if max_iter == 1 else self.epochs
        for ep in range(epochs):
            model.train()
            run_loss = 0.0
            for xb, yb in loader:
                opt.zero_grad()
                pred = model(xb)
                loss = self.pinball(pred, yb)
                loss.backward()
                opt.step()
                run_loss += loss.item() * len(xb)
            run_loss /= len(ds)
            if run_loss + 1e-9 < best_loss:
                best_loss = run_loss
                best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}
                no_improve = 0
            else:
                no_improve += 1
                if self.patience and no_improve >= self.patience:
                    break
        if best_state is not None:
            model.load_state_dict(best_state)
        self.model = model
        return self

    def predict(self, df: pd.DataFrame, feature_cols: List[str]) -> np.ndarray:
        X = df[feature_cols].astype(float).values
        X_t = torch.from_numpy(X).float()
        with torch.no_grad():
            y = self.model(X_t).cpu().numpy().reshape(-1)
        return y

    def save(self, path: Path) -> None:
        path.parent.mkdir(parents=True, exist_ok=True)
        meta = {
            "quantile": self.quantile,
            "units": self.units,
            "in_dim": self.in_dim,
        }
        torch.save({"state_dict": self.model.state_dict(), "meta": meta}, path)

    @classmethod
    def load(cls, path: Path) -> "NNQuantileModel":
        ckpt = torch.load(path, map_location="cpu")
        meta = ckpt["meta"]
        m = cls(quantile=meta["quantile"], units=meta["units"], lr=1e-3, epochs=1, batch_size=32, patience=0, l2_penalty=0.0)
        m.in_dim = meta["in_dim"]
        m.model = MLP(m.in_dim, m.units)
        m.model.load_state_dict(ckpt["state_dict"]) 
        return m


# ----------------------------- Task & Progress -----------------------------

@dataclass(frozen=True)
class TaskKey:
    model: str
    quantile: float
    horizon: int
    country: str
    window_start: str
    window_end: str
    nn_version: Optional[str] = None  # reserved for future

    def id(self) -> str:
        return f"{self.model}|q={self.quantile}|h={self.horizon}|{self.country}|{self.window_start}_{self.window_end}"


PROGRESS_COLUMNS = [
    "MODEL", "QUANTILE", "HORIZON", "COUNTRY", "WINDOW_START", "WINDOW_END",
    "STATUS", "LAST_UPDATE", "ERROR_MSG", "MODEL_PATH", "FORECAST_ROWS"
]


def load_progress(progress_path: Path) -> pd.DataFrame:
    if progress_path.exists():
        return pd.read_parquet(progress_path)
    else:
        return pd.DataFrame(columns=PROGRESS_COLUMNS)


def update_progress_row(lock: FileLock, progress_path: Path, row: Dict[str, Any]) -> None:
    with lock:
        df = load_progress(progress_path)
        mask = (
            (df["MODEL"] == row["MODEL"]) &
            (df["QUANTILE"] == row["QUANTILE"]) &
            (df["HORIZON"] == row["HORIZON"]) &
            (df["COUNTRY"] == row["COUNTRY"]) &
            (df["WINDOW_START"] == row["WINDOW_START"]) &
            (df["WINDOW_END"] == row["WINDOW_END"]) 
        )
        df = df.loc[~mask]
        df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)
        df.to_parquet(progress_path, index=False)


def append_error(errors_path: Path, lock: FileLock, row: Dict[str, Any]) -> None:
    with lock:
        if errors_path.exists():
            df = pd.read_parquet(errors_path)
        else:
            df = pd.DataFrame(columns=list(row.keys()))
        df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)
        df.to_parquet(errors_path, index=False)


# ----------------------------- Forecast Assembly -----------------------------

def append_forecasts(global_path: Path, rows: pd.DataFrame) -> None:
    """Append rows to parquet in a locked and atomic way."""
    safe_mkdirs(global_path.parent)
    lock = FileLock(str(lock_path(global_path)))
    tmp = global_path.parent / f"staging_{uuid.uuid4().hex}.parquet"
    rows = rows.copy()
    rows.reset_index(drop=True, inplace=True)
    with lock:
        # write staging
        rows.to_parquet(tmp, index=False)
        # merge: if file exists, read existing and concat, drop duplicates
        if global_path.exists():
            existing = pq.ParquetFile(global_path)
            tbl = existing.read()
            df_old = tbl.to_pandas()
            df_new = pd.concat([df_old, rows], ignore_index=True)
            # de-dup by TIME-COUNTRY
            df_new.drop_duplicates(subset=["TIME", "COUNTRY", "HORIZON", "QUANTILE"], keep="last", inplace=True)
            df_new.sort_values(["TIME", "COUNTRY", "HORIZON", "QUANTILE"], inplace=True)
            df_new.to_parquet(global_path, index=False)
        else:
            os.rename(tmp, global_path)
        if tmp.exists():
            try:
                os.remove(tmp)
            except Exception:
                pass


# ----------------------------- Worker -----------------------------

def worker_initializer():
    # Limit BLAS threads per process to avoid oversubscription
    os.environ.setdefault("OMP_NUM_THREADS", "1")
    os.environ.setdefault("OPENBLAS_NUM_THREADS", "1")
    os.environ.setdefault("MKL_NUM_THREADS", "1")
    os.environ.setdefault("VECLIB_MAXIMUM_THREADS", "1")
    os.environ.setdefault("NUMEXPR_NUM_THREADS", "1")


def estimate_memory_mb(func, *args, **kwargs) -> int:
    proc = psutil.Process(os.getpid())
    rss_before = proc.memory_info().rss
    try:
        func(*args, **kwargs)
    except Exception:
        # Estimation may fail—still measure
        pass
    gc.collect()
    rss_after = proc.memory_info().rss
    delta = max(0, rss_after - rss_before)
    return int(delta / (1024 * 1024))


# Core worker that trains/loads model + predicts for one task

def execute_task(task: TaskKey, cfg: Dict[str, Any], paths: Dict[str, Path]) -> Tuple[TaskKey, str, Optional[str], int, Optional[pd.DataFrame]]:
    """Return (task, status, err_msg, n_rows, forecast_df or None)"""
    # Paths & locks
    progress_lock = FileLock(str(lock_path(paths["progress_parquet"])))
    errors_lock = FileLock(str(lock_path(paths["errors_parquet"])))

    def mark(status: str, err: Optional[str] = None, model_path: Optional[Path] = None, n_rows: int = 0):
        row = {
            "MODEL": task.model,
            "QUANTILE": task.quantile,
            "HORIZON": task.horizon,
            "COUNTRY": task.country,
            "WINDOW_START": task.window_start,
            "WINDOW_END": task.window_end,
            "STATUS": status,
            "LAST_UPDATE": datetime.utcnow().isoformat(),
            "ERROR_MSG": err,
            "MODEL_PATH": str(model_path) if model_path else None,
            "FORECAST_ROWS": n_rows,
        }
        update_progress_row(progress_lock, paths["progress_parquet"], row)
        if err:
            append_error(paths["errors_parquet"], errors_lock, row)

    try:
        mark("training")
        # Load data for country
        country_file = paths["countries"][task.country]
        df = read_df(country_file)
        df = ensure_time_index(df, "TIME")
        # Build features
        target = cfg['data']['target']
        lags = cfg['data']['lags'] or list(range(1, int(cfg.get('ar_qr', {}).get('p', 12)) + 1))
        base_cols = [target]
        df = add_lags(df, target, lags)
        feature_cols = [f"{target}_lag{L}" for L in lags]
        if cfg['data'].get('country_dummies', False) and 'COUNTRY' in df.columns:
            dummies = pd.get_dummies(df['COUNTRY'], prefix='C')
            df = pd.concat([df, dummies], axis=1)
            feature_cols += list(dummies.columns)
        # Trim missing from lags
        if cfg['data'].get('trimming', True):
            df = df.dropna(subset=feature_cols + base_cols)
        # Split by window
        time_mask = (df['TIME'] >= pd.Timestamp(task.window_start)) & (df['TIME'] <= pd.Timestamp(task.window_end))
        train_df = df.loc[time_mask].copy()
        # Ensure enough points
        if len(train_df) < cfg['splits']['min_train_points']:
            raise RuntimeError(f"Insufficient train points: {len(train_df)} < {cfg['splits']['min_train_points']}")
        # The forecast feature row is at window_end (using last lags)
        # Build a 1-row DF with features at window_end to forecast at t + horizon
        # Here we simply take the last row of train_df for features
        feat_row = df.loc[df['TIME'] == pd.Timestamp(task.window_end)]
        if feat_row.empty:
            raise RuntimeError("Feature row at window_end not found")
        # Select model
        model_dir = paths['models_root'] / task.model / f"country={task.country}" / f"q={task.quantile}" / f"h={task.horizon}" / f"window_{task.window_start}_{task.window_end}"
        safe_mkdirs(model_dir)
        # File paths
        if task.model == 'ensemble_nn':
            model_path = model_dir / "model.pt"
        else:
            model_path = model_dir / "model.json"

        # Optional reload
        if cfg['runtime'].get('allow_reload', True) and not cfg['runtime'].get('retrain_if_exists', False) and model_path.exists():
            # Load and predict
            if task.model == 'ensemble_nn':
                model = NNQuantileModel.load(model_path)
            elif task.model in ('lqr', 'ar-qr'):
                model = LQRModel.load(model_path)
            else:
                raise ValueError(f"Unknown model {task.model}")
        else:
            # Fit model
            if task.model == 'ensemble_nn':
                # For ensemble, we train 'parallel_models' and average outputs for the same quantile
                nn_cfg = cfg['ensemble_nn']
                q_model = None
                preds = []
                # Train a single network for artifact; ensemble averaged at predict-time
                q_model = NNQuantileModel(
                    quantile=task.quantile,
                    units=nn_cfg['units_per_layer'],
                    lr=float(nn_cfg['learning_rate']),
                    epochs=int(nn_cfg['epochs']),
                    batch_size=int(nn_cfg['batch_size']),
                    patience=int(nn_cfg['patience']),
                    l2_penalty=float(nn_cfg['l2_penalty']),
                )
                # If in memory-probe stage, caller may pass max_iter=1; here we just fit normally
                q_model.fit(train_df, feature_cols, target)
                model = q_model
                model.save(model_path)
            elif task.model == 'lqr':
                lqr_cfg = cfg['lqr']
                model = LQRModel(quantile=task.quantile, rff_features=int(lqr_cfg.get('rff_features', 0)))
                model.fit(train_df, feature_cols, target)
                model.save(model_path)
            elif task.model == 'ar-qr':
                ar_cfg = cfg['ar_qr']
                model = ARQRModel(quantile=task.quantile, rff_features=0)
                model.fit(train_df, feature_cols, target)
                model.save(model_path)
            else:
                raise ValueError(f"Unknown model {task.model}")

        # Predict
        yhat = model.predict(feat_row, feature_cols)
        forecast_time = pd.Timestamp(task.window_end)  # we treat emitted forecast aligned to feature time; downstream we shift to horizon target time
        # Build forecast row: target time is window_end + horizon steps ahead
        # We need the actual TRUE_DATA at that time if available
        # Find time at horizon ahead
        # Simplify: use the df index to locate time horizon ahead
        idx = df.index[df['TIME'] == pd.Timestamp(task.window_end)]
        if len(idx) == 0:
            raise RuntimeError("window_end index not found")
        i = idx[0]
        k = i + task.horizon
        if k >= len(df):
            raise RuntimeError("Forecast horizon goes beyond data range")
        t_target = df.iloc[k]['TIME']
        true_val = float(df.iloc[k][target]) if target in df.columns else np.nan
        out = pd.DataFrame({
            'TIME': [t_target],
            'COUNTRY': [task.country],
            'TRUE_DATA': [true_val],
            'FORECAST': [float(yhat.reshape(-1)[0])],
            'HORIZON': [task.horizon],
            'QUANTILE': [task.quantile],
            'MODEL': [task.model],
            'WINDOW_START': [task.window_start],
            'WINDOW_END': [task.window_end],
        })

        mark("done", err=None, model_path=model_path, n_rows=len(out))
        return task, "done", None, len(out), out
    except Exception as e:
        err = f"{type(e).__name__}: {e}"
        mark("failed", err=err, model_path=None, n_rows=0)
        return task, "failed", err, 0, None


# ----------------------------- Planning & Scheduling -----------------------------

@dataclass
class PlannedTask:
    key: TaskKey
    mem_est_mb: int


def plan_tasks(cfg: Dict[str, Any], io_paths: Dict[str, Path]) -> Tuple[List[PlannedTask], Dict[str, Path]]:
    countries = load_country_files(io_paths['data_path'])
    io_paths['countries'] = countries

    data_target = cfg['data']['target']
    horizons = cfg['data']['horizons']
    quantiles = cfg['data']['quantiles']

    # Determine date range per country; build windows using test_cutoff and rolling_window
    rw = cfg['rolling_window']
    size = int(rw['size'])
    step = int(rw['step'])

    planned: List[PlannedTask] = []

    for country, path in countries.items():
        df = read_df(path)
        df = ensure_time_index(df, 'TIME')
        dates = df['TIME']
        test_cutoff = pd.Timestamp(cfg['splits']['test_cutoff'])
        start = dates.iloc[0]
        end = dates.iloc[-1]
        start_date = test_cutoff if rw.get('start', 'auto') == 'auto' else pd.Timestamp(rw['start'])
        end_date = end if rw.get('end', 'auto') == 'auto' else pd.Timestamp(rw['end'])
        for h in horizons:
            windows = generate_windows(dates, size=size, step=step, start_date=start_date, end_date=end_date, horizon=h)
            for (wstart, wend, ftime) in windows:
                for q in quantiles:
                    # Which models are enabled
                    for model_key, enabled in [("ar-qr", cfg.get('ar_qr', {}).get('enabled', False)),
                                               ("lqr", cfg.get('lqr', {}).get('enabled', False)),
                                               ("ensemble_nn", cfg.get('ensemble_nn', {}).get('enabled', False))]:
                        if not enabled:
                            continue
                        tk = TaskKey(model=model_key, quantile=float(q), horizon=int(h), country=country,
                                     window_start=str(wstart.date()), window_end=str(wend.date()))
                        # memory estimate placeholder; refined later
                        planned.append(PlannedTask(key=tk, mem_est_mb=0))
    return planned, io_paths


def estimate_memory_for_tasks(tasks: List[PlannedTask], cfg: Dict[str, Any], paths: Dict[str, Path]) -> None:
    # We'll estimate per (model) using the first occurrence per country to avoid huge overhead
    grouped: Dict[Tuple[str, str], List[PlannedTask]] = {}
    for pt in tasks:
        grouped.setdefault((pt.key.model, pt.key.country), []).append(pt)

    for (model, country), pts in tqdm(grouped.items(), desc="Memory probe", total=len(grouped)):
        # Pick first task for this (model,country)
        sample = pts[0]
        # Run execute_task with a minimal training iteration: for NN we pass max_iter=1 via config clone
        # To do that cleanly, we simply load df and run model.fit(..., max_iter=1) in-process and measure RSS
        def probe():
            # mimic inner of execute_task up to fit
            country_file = paths['countries'][country]
            df = read_df(country_file)
            df = ensure_time_index(df, 'TIME')
            target = cfg['data']['target']
            lags = cfg['data']['lags'] or list(range(1, int(cfg.get('ar_qr', {}).get('p', 12)) + 1))
            df = add_lags(df, target, lags).dropna()
            feature_cols = [f"{target}_lag{L}" for L in lags]
            # window slice
            time_mask = (df['TIME'] >= pd.Timestamp(sample.key.window_start)) & (df['TIME'] <= pd.Timestamp(sample.key.window_end))
            train_df = df.loc[time_mask]
            if sample.key.model == 'ensemble_nn':
                nn_cfg = cfg['ensemble_nn']
                m = NNQuantileModel(sample.key.quantile, nn_cfg['units_per_layer'], float(nn_cfg['learning_rate']), int(nn_cfg['epochs']), int(nn_cfg['batch_size']), int(nn_cfg['patience']), float(nn_cfg['l2_penalty']))
                m.fit(train_df, feature_cols, target, max_iter=1)
            else:
                lqr_cfg = cfg['lqr']
                m = LQRModel(sample.key.quantile, rff_features=int(lqr_cfg.get('rff_features', 0)))
                m.fit(train_df, feature_cols, target, max_iter=1)

        mb = estimate_memory_mb(probe)
        mb += int(cfg['runtime'].get('mem_probe_fudge_mb', 200))
        for pt in pts:
            pt.mem_est_mb = max(mb, 100)  # at least 100MB


class MemoryGate:
    def __init__(self, total_mb: int):
        self.total = total_mb
        self.available = total_mb
        self.cv = threading.Condition()

    def acquire(self, need_mb: int):
        with self.cv:
            while need_mb > self.available:
                self.cv.wait()
            self.available -= need_mb

    def release(self, mb: int):
        with self.cv:
            self.available += mb
            if self.available > self.total:
                self.available = self.total
            self.cv.notify_all()


def run_scheduler(planned: List[PlannedTask], cfg: Dict[str, Any], paths: Dict[str, Path]) -> None:
    # Progress/Data paths
    progress_path = paths['progress_parquet']
    errors_path = paths['errors_parquet']
    safe_mkdirs(progress_path.parent)
    safe_mkdirs(errors_path.parent)

    # Filter planned by progress & cache state
    progress_df = load_progress(progress_path)

    filtered: List[PlannedTask] = []
    for pt in planned:
        tk = pt.key
        # Skip done rows
        mask = (
            (progress_df['MODEL'] == tk.model) & (progress_df['QUANTILE'] == tk.quantile) & (progress_df['HORIZON'] == tk.horizon) &
            (progress_df['COUNTRY'] == tk.country) & (progress_df['WINDOW_START'] == tk.window_start) & (progress_df['WINDOW_END'] == tk.window_end) &
            (progress_df['STATUS'] == 'done')
        )
        if progress_df.shape[0] and mask.any():
            continue
        # Respect cache if model exists and allow_reload
        model_dir = paths['models_root'] / tk.model / f"country={tk.country}" / f"q={tk.quantile}" / f"h={tk.horizon}" / f"window_{tk.window_start}_{tk.window_end}"
        model_path = model_dir / ("model.pt" if tk.model == 'ensemble_nn' else 'model.json')
        if cfg['runtime'].get('allow_reload', True) and not cfg['runtime'].get('retrain_if_exists', False) and model_path.exists():
            # Mark done immediately and skip training
            row = {
                "MODEL": tk.model, "QUANTILE": tk.quantile, "HORIZON": tk.horizon, "COUNTRY": tk.country,
                "WINDOW_START": tk.window_start, "WINDOW_END": tk.window_end, "STATUS": "done",
                "LAST_UPDATE": datetime.utcnow().isoformat(), "ERROR_MSG": None, "MODEL_PATH": str(model_path), "FORECAST_ROWS": 1
            }
            update_progress_row(FileLock(str(lock_path(progress_path))), progress_path, row)
            continue
        filtered.append(pt)

    if not filtered:
        logging.info("Nothing to do — all tasks complete or cached.")
        return

    # Resource caps
    detected_cores = detect_cpu_cores()
    cores_cfg = cfg['runtime'].get('max_cores', 'auto')
    max_workers = detected_cores if cores_cfg == 'auto' else int(cores_cfg)

    avail_ram = detect_available_ram_bytes()
    ram_cfg = cfg['runtime'].get('max_ram_gb', 'auto')
    if ram_cfg == 'auto':
        cap_bytes = int(avail_ram * float(cfg['runtime'].get('safety_ram_fraction', 0.8)))
    else:
        cap_bytes = int(float(ram_cfg) * (1024 ** 3))
    mem_gate = MemoryGate(total_mb=int(cap_bytes / (1024 * 1024)))

    # Sort tasks by mem_est descending to pack better
    filtered.sort(key=lambda pt: pt.mem_est_mb,    reverse=True)

    attempts: Dict[str, int] = {}
    next_idx = 0
    running: Dict[Any, Tuple[PlannedTask, int]] = {}

    # build dashboard helper inside to avoid globals
    def refresh_dashboard():
        try:
            build_dashboard(paths)
        except Exception as e:
            logging.warning(f"Dashboard refresh failed: {e}")

    with ProcessPoolExecutor(max_workers=max_workers, initializer=worker_initializer) as pool:
        last_refresh = time.time()
        pbar = tqdm(total=len(filtered), desc="Tasks", unit="task")
        while next_idx < len(filtered) or running:
            # launch while capacity on cores and memory
            launched_any = False
            while next_idx < len(filtered) and len(running) < max_workers:
                pt = filtered[next_idx]
                need = pt.mem_est_mb
                # check memory gate non-blocking
                if need <= mem_gate.available:
                    mem_gate.acquire(need)
                    tk = pt.key
                    # prepare minimal paths mapping for the worker
                    worker_paths = {
                        "progress_parquet": paths["progress_parquet"],
                        "errors_parquet": paths["errors_parquet"],
                        "models_root": paths["models_root"],
                        "countries": paths["countries"],
                    }
                    fut = pool.submit(execute_task, tk, cfg, worker_paths)
                    running[fut] = (pt, need)
                    next_idx += 1
                    launched_any = True
                else:
                    break  # wait for completions to free memory

            # collect one completion (or periodic refresh)
            timeout = max(0.5, float(cfg["runtime"].get("progress_refresh_sec", 5)))
            done_any = False
            for fut in as_completed(list(running.keys()), timeout=timeout):
                pt, need = running.pop(fut)
                mem_gate.release(need)
                done_any = True
                try:
                    task, status, err, n_rows, rows = fut.result()
                except Exception as e:
                    task = pt.key
                    status = "failed"
                    err = f"{type(e).__name__}: {e}"
                    n_rows = 0
                    rows = None

                # handle result
                if status == "done" and rows is not None and n_rows > 0:
                    # append to global parquet for (q,h)
                    q = task.quantile
                    h = task.horizon
                    forecast_path = paths["forecasts_root"] / f"q={q}" / f"h={h}" / "rolling_window.parquet"
                    append_forecasts(forecast_path, rows)
                    pbar.update(1)
                else:
                    # retry?
                    tid = task.id()
                    a = attempts.get(tid, 0)
                    if a < int(cfg["runtime"].get("retries", 1)):
                        attempts[tid] = a + 1
                        # re-enqueue the same planned task to the tail
                        filtered.append(pt)
                    pbar.update(1)

                # periodic dashboard refresh after each completion
                refresh_dashboard()
                break  # collect only one to interleave submits

            # If nothing completed and nothing launched, still refresh dashboard periodically
            if not done_any and not launched_any and (time.time() - last_refresh) >= timeout:
                refresh_dashboard()
                last_refresh = time.time()

        pbar.close()

    logging.info("Scheduling finished.")
    refresh_dashboard()


# ----------------------------- Dashboard -----------------------------

def build_dashboard(paths: Dict[str, Path]) -> None:
    """Simple static HTML dashboard summarizing progress."""
    prog_p = paths["progress_parquet"]
    err_p = paths["errors_parquet"]
    df = pd.read_parquet(prog_p) if prog_p.exists() else pd.DataFrame(columns=PROGRESS_COLUMNS)
    de = pd.read_parquet(err_p) if err_p.exists() else pd.DataFrame()

    counts = df["STATUS"].value_counts().to_dict() if not df.empty else {}
    total = int(df.shape[0])
    done = int(counts.get("done", 0))
    training = int(counts.get("training", 0))
    pending = int(counts.get("pending", 0))
    failed = int(counts.get("failed", 0))

    html = f"""<!doctype html>
<html><head><meta charset="utf-8">
<title>Quant Forecast Progress</title>
<style>
body {{ font-family: system-ui, -apple-system, Segoe UI, Roboto, Arial; margin:24px; }}
.grid {{ display:grid; grid-template-columns: repeat(auto-fit, minmax(180px,1fr)); gap:12px; }}
.kpi {{ background:#f7f7f8; border:1px solid #e5e7eb; border-radius:12px; padding:16px; text-align:center; }}
.kpi .n {{ font-size:28px; font-weight:700; }}
table {{ border-collapse: collapse; width:100%; font-size:12px; }}
th, td {{ border:1px solid #e5e7eb; padding:6px 8px; }}
th {{ background:#fafafa; }}
.badge {{ padding:2px 8px; border-radius:12px; color:white; font-weight:600; }}
.badge.done {{ background:#10b981; }}
.badge.training {{ background:#3b82f6; }}
.badge.pending {{ background:#9ca3af; }}
.badge.failed {{ background:#ef4444; }}
.small {{ color:#6b7280; font-size:12px; }}
</style></head>
<body>
<h2>Quantile Forecast Runner <span class="small">Updated {datetime.utcnow().isoformat()} UTC</span></h2>
<div class="grid">
  <div class="kpi"><div class="n">{total}</div><div>Total tasks</div></div>
  <div class="kpi"><div class="n">{done}</div><div>Done</div></div>
  <div class="kpi"><div class="n">{training}</div><div>Training</div></div>
  <div class="kpi"><div class="n">{pending}</div><div>Pending</div></div>
  <div class="kpi"><div class="n">{failed}</div><div>Failed</div></div>
</div>
<h3>Recent Activity</h3>
<table>
<tr><th>Status</th><th>Model</th><th>Q</th><th>H</th><th>Country</th><th>Win Start</th><th>Win End</th><th>Updated</th><th>Error</th></tr>
"""
    if not df.empty:
        tail = df.sort_values("LAST_UPDATE").tail(300)
        for _, r in tail.iterrows():
            st = r["STATUS"]
            badge = f"<span class='badge {st}'>{st}</span>"
            html += f"<tr><td>{badge}</td><td>{r['MODEL']}</td><td>{r['QUANTILE']}</td><td>{r['HORIZON']}</td><td>{r['COUNTRY']}</td><td>{r['WINDOW_START']}</td><td>{r['WINDOW_END']}</td><td>{r['LAST_UPDATE']}</td><td>{(r.get('ERROR_MSG') or '')[:120]}</td></tr>\n"
    html += "</table>\n"

    if not de.empty:
        html += "<h3>Errors</h3><table><tr><th>When</th><th>Model</th><th>Q</th><th>H</th><th>Country</th><th>Window</th><th>Error</th></tr>"
        # normalize columns
        if "WHEN" not in de.columns:
            de["WHEN"] = de.get("LAST_UPDATE", "")
        for _, r in de.tail(200).iterrows():
            html += f"<tr><td>{r.get('WHEN','')}</td><td>{r.get('MODEL','')}</td><td>{r.get('QUANTILE','')}</td><td>{r.get('HORIZON','')}</td><td>{r.get('COUNTRY','')}</td><td>{r.get('WINDOW_START','')}–{r.get('WINDOW_END','')}</td><td>{(r.get('ERROR') or r.get('ERROR_MSG') or '')[:160]}</td></tr>"
        html += "</table>"

    html += "</body></html>"

    dash_path = paths["dashboard_html"]
    dash_path.parent.mkdir(parents=True, exist_ok=True)
    dash_path.write_text(html)


# ----------------------------- CLI & Entry -----------------------------

def make_paths(cfg: Dict[str, Any]) -> Dict[str, Path]:
    root = Path(cfg["io"]["output_root"])
    paths: Dict[str, Path] = {
        "data_path": Path(cfg["io"].get("data_path", cfg["io"]["data_path"])),
        "output_root": root,
        "models_root": root / cfg["io"]["models_dir"],
        "forecasts_root": root / cfg["io"]["forecasts_dir"],
        "progress_parquet": root / cfg["io"]["progress_parquet"],
        "errors_parquet": root / cfg["io"]["errors_parquet"],
        "dashboard_html": root / cfg["io"]["dashboard_html"],
        "logs_dir": root / cfg["io"]["logs_dir"],
    }
    # ensure dirs
    for k in ("models_root", "forecasts_root", "progress_parquet", "errors_parquet", "dashboard_html", "logs_dir"):
        p = paths[k]
        if k.endswith("_parquet"):
            p.parent.mkdir(parents=True, exist_ok=True)
        elif k.endswith("_html"):
            p.parent.mkdir(parents=True, exist_ok=True)
        else:
            p.mkdir(parents=True, exist_ok=True)
    return paths


def main(argv: Optional[List[str]] = None) -> int:
    import argparse
    ap = argparse.ArgumentParser("Quantile Forecast Runner (CPU-only, SLURM-aware)")
    ap.add_argument("--write-config", type=str, help="Write default config to this path and exit")
    ap.add_argument("--config", type=str, help="Path to YAML config")
    ap.add_argument("--dry-run", action="store_true", help="Plan and memory-probe only")
    ap.add_argument("--max-workers", type=int, default=None, help="Override max workers (cores)")
    args = ap.parse_args(argv)

    if args.write_config:
        out = Path(args.write_config)
        write_text(out, DEFAULT_CONFIG_YAML.strip() + "\n")
        print(f"Wrote default config to {out}")
        return 0

    if not args.config:
        print("ERROR: --config is required (or use --write-config).", file=sys.stderr)
        return 2

    cfg = read_yaml(Path(args.config))
    # Fill IO defaults if user provided the earlier YAML layout
    if "io" not in cfg:
        # backfill from user's schema
        cfg["io"] = {
            "data_path": cfg["data"]["path"],
            "output_root": cfg.get("output", {}).get("root", "out"),
            "models_dir": "models",
            "forecasts_dir": "forecasts",
            "progress_dir": "progress",
            "logs_dir": "logs",
            "dashboard_html": "progress/dashboard.html",
            "progress_parquet": "progress/progress.parquet",
            "errors_parquet": "progress/errors.parquet",
        }
    if "runtime" not in cfg:
        cfg["runtime"] = {}
    cfg["runtime"].setdefault("retries", 1)
    cfg["runtime"].setdefault("safety_ram_fraction", 0.8)
    cfg["runtime"].setdefault("max_cores", "auto")
    cfg["runtime"].setdefault("max_ram_gb", "auto")
    cfg["runtime"].setdefault("mem_probe_fudge_mb", 200)

    paths = make_paths(cfg)
    setup_logging(paths["logs_dir"])
    set_global_seeds(int(cfg.get("seed", 123)))

    # MKL/OMP pinning in parent as well
    os.environ.setdefault("OMP_NUM_THREADS", "1")
    os.environ.setdefault("OPENBLAS_NUM_THREADS", "1")
    os.environ.setdefault("MKL_NUM_THREADS", "1")
    os.environ.setdefault("VECLIB_MAXIMUM_THREADS", "1")
    os.environ.setdefault("NUMEXPR_NUM_THREADS", "1")

    # Plan tasks
    planned, paths = plan_tasks(cfg, paths)
    if not planned:
        logging.info("No tasks planned (check dates / horizons / data).")
        return 0

    # Memory probe
    logging.info("Estimating memory usage per model/country group...")
    estimate_memory_for_tasks(planned, cfg, paths)

    # Dry run?
    if args.dry_run:
        print("---- DRY RUN (first 10 tasks) ----")
        for pt in planned[:10]:
            print({
                "MODEL": pt.key.model,
                "Q": pt.key.quantile,
                "H": pt.key.horizon,
                "COUNTRY": pt.key.country,
                "WIN_START": pt.key.window_start,
                "WIN_END": pt.key.window_end,
                "EST_MB": pt.mem_est_mb,
            })
        # also write dashboard skeleton
        build_dashboard(paths)
        return 0

    # Run
    run_scheduler(planned, cfg, paths)
    logging.info("Done.")
    return 0


if __name__ == "__main__":
    sys.exit(main())


# here is run_analysis.py
#!/usr/bin/env python3
"""
Cross-Country Quantile Forecasting with Factor Neural Networks

This script demonstrates a complete end-to-end workflow for multi-horizon quantile forecasting 
across countries using Factor Neural Networks (FNN) and Linear Quantile Regression (LQR).

Converted from Jupyter notebook to Python script with file output and plot saving.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from pathlib import Path
import yaml
from datetime import datetime
import sys
import os
import traceback
from scipy import stats

# Set up plotting
plt.style.use('default')
sns.set_palette("husl")
warnings.filterwarnings('ignore')

# Create output directories
output_dir = Path("analysis_output")
plots_dir = output_dir / "plots"
data_dir = output_dir / "data"
logs_dir = output_dir / "logs"

for dir_path in [output_dir, plots_dir, data_dir, logs_dir]:
    dir_path.mkdir(exist_ok=True)

# Set up logging to file
import logging
log_file = logs_dir / f"analysis_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file),
        logging.StreamHandler(sys.stdout)
    ]
)

def save_output(content, filename, subdir=""):
    """Save text content to file"""
    if subdir:
        save_path = output_dir / subdir / filename
        (output_dir / subdir).mkdir(exist_ok=True)
    else:
        save_path = output_dir / filename
    
    with open(save_path, 'w') as f:
        f.write(content)
    logging.info(f"Saved output to {save_path}")

def save_plot(fig, filename, subdir="plots"):
    """Save matplotlib figure to file"""
    save_path = output_dir / subdir / filename
    (output_dir / subdir).mkdir(exist_ok=True)
    fig.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close(fig)
    logging.info(f"Saved plot to {save_path}")

def save_dataframe(df, filename, subdir="data"):
    """Save dataframe to CSV"""
    save_path = output_dir / subdir / filename
    (output_dir / subdir).mkdir(exist_ok=True)
    df.to_csv(save_path, index=False)
    logging.info(f"Saved dataframe to {save_path}")

def main():
    """Main analysis function"""
    

    # chekc avialability of GPU 



    logging.info("Starting Cross-Country Quantile Forecasting Analysis")
    logging.info("="*70)
    
    # Project imports (adjust paths as needed)
    sys.path.append('.')
    
    try:
        from src.utils import (
            load_country_data, 
            load_config,
            set_seeds,
            handle_missing_values,
            scale_features,
            create_lagged_features,
            create_forecast_targets,
            create_time_split
        )
        
        # Import high-level API classes
        from src.ensemble_nn_api import EnsembleNNAPI
        from src.lqr_api import LQRModel
        
        from src.metrics import (
            pinball_loss,
            compute_quantile_losses,
            create_loss_summary_table,
            dm_test_by_groups,
            multiple_testing_correction
        )
        
        from src.evaluation import (
            create_model_comparison_table,
            plot_training_curves,
            plot_forecast_paths,
            plot_loss_comparison,
            plot_calibration,
            create_evaluation_dashboard
        )
        
        import torch 
        torch.set_num_threads(4)
        
        logging.info("Libraries imported successfully!")
        logging.info(f"Numpy version: {np.__version__}")
        logging.info(f"Pandas version: {pd.__version__}")
        
    except ImportError as e:
        logging.error(f"Import error: {e}")
        return
    
    # ===== CONFIGURATION AND SETUP =====
    logging.info("\n" + "="*50)
    logging.info("CONFIGURATION AND SETUP")
    logging.info("="*50)
    
    config_path = "configs/experiment_big_data.yaml"
    
    try:
        config = load_config(config_path)
        logging.info("Configuration loaded successfully!")
        config_summary = f"""
Key parameters:
Target variable: {config['data']['target']}
Quantiles: {config['data']['quantiles']}
Horizons: {config['data']['horizons']}
Lags: {config['data']['lags']}
Validation size: {config['splits']['validation_size']}
"""
        logging.info(config_summary)
        save_output(config_summary, "config_summary.txt")
        
    except FileNotFoundError:
        logging.info(f"Configuration file not found at {config_path}")
        logging.info("Using default configuration...")
        
        # Default configuration
        config = {
            'seed': 123,
            'data': {
                'target': 'GDP',
                'required_columns': ['TIME', 'CISS', 'INFL_OECD', 'GDP', 'CREDIT_TO_GDP'],
                'quantiles': [0.05, 0.95],
                'horizons': [1],
                'lags': [1],
                'missing': 'forward_fill_then_mean',
                'scale': 'per_country',
                'scale_target': False,
                'trimming': None
            },
            'splits': {
                'validation_size': 0.2,
                'train_start': '1990-12-31',
                'test_cutoff': '2018-12-31',
                'min_train_points': 60
            },
            'ensemble_nn': {
                'units_per_layer': [8, 4],
                'activation': 'relu',
                'optimizer': 'adam',
                'learning_rate': 1e-3,
                'epochs': 50,
                'batch_size': 64,
                'patience': 10,
                'parallel_models': 1,
                'device': 'auto',
                'per_country': True,
                'country_dummies': False,
                'l2_penalty': 0.001
            },
            'lqr': {
                'alphas': [0.0, 0.1, 1.0, 10.0],
                'solver': 'huberized'
            }
        }
    
    # Set random seed
    set_seeds(config['seed'])
    logging.info(f"Random seed set to: {config['seed']}")
    
    # ===== LOAD AND EXPLORE DATASET =====
    logging.info("\n" + "="*50)
    logging.info("LOAD AND EXPLORE DATASET")
    logging.info("="*50)
    
    data_path = "processed_per_country"
    
    try:
        # Load all country CSV files
        country_data = load_country_data(
            data_path=data_path,
            required_columns=config['data']['required_columns']
        )
        
        data_summary = f"Successfully loaded data for {len(country_data)} countries:\n"
        for country, df in country_data.items():
            data_summary += f"  {country}: {len(df)} observations, {df['TIME'].min()} to {df['TIME'].max()}\n"
        
        logging.info(data_summary)
        save_output(data_summary, "data_loading_summary.txt")
        
        # Save sample data
        sample_country = list(country_data.keys())[0]
        sample_df = country_data[sample_country].head(10)
        save_dataframe(sample_df, f"sample_data_{sample_country}.csv")
        
        # Save summary statistics
        summary_stats = country_data[sample_country].describe()
        save_dataframe(summary_stats, f"summary_stats_{sample_country}.csv")
        
    except FileNotFoundError:
        logging.info(f"Data directory not found at {data_path}")
        logging.info("Creating synthetic data for demonstration...")
        
        # Create synthetic data for demonstration
        countries = ['USA', 'DEU', 'FRA', 'GBR']
        start_date = '2000-01-01'
        end_date = '2023-12-31'
        freq = 'Q'  # Quarterly data
        
        country_data = {}
        np.random.seed(config['seed'])
        
        for country in countries:
            dates = pd.date_range(start_date, end_date, freq=freq)
            n_obs = len(dates)
            
            # Generate synthetic time series with trends and cycles
            t = np.arange(n_obs)
            trend = 0.01 * t + np.random.normal(0, 0.1, n_obs).cumsum()
            
            df = pd.DataFrame({
                'TIME': dates,
                'GDP': 100 + trend + np.random.normal(0, 2, n_obs),
                'CISS': np.random.beta(2, 5, n_obs),
                'INFL_OECD': np.random.normal(2, 1, n_obs),
                'CREDIT_TO_GDP': 150 + 10 * np.sin(t / 20) + np.random.normal(0, 5, n_obs)
            })
            
            country_data[country] = df
        
        logging.info(f"Created synthetic data for {len(country_data)} countries")
        
        # Save synthetic data
        for country, df in country_data.items():
            save_dataframe(df, f"synthetic_data_{country}.csv")
    
    # Update config with actual columns
    config['data']['required_columns'] = list(country_data[list(country_data.keys())[0]].columns)
    #logging.info(f"Updated required columns in config: {config['data']['required_columns']}")
    
    # Visualize the data
    variables = config['data']['required_columns'][1:]  # Exclude 'TIME'
    colors = plt.cm.Set1(np.linspace(0, 1, len(country_data)))
    
    fig, axes = plt.subplots(3, 2, figsize=(15, 10))
    axes = axes.flatten()
    
    for i, var in enumerate(variables[:6]):
        ax = axes[i]
        
        for j, (country, df) in enumerate(country_data.items()):
            ax.plot(df['TIME'], df[var], label=country, color=colors[j], linewidth=2)
        
        ax.set_title(f'{var} Across Countries')
        ax.set_xlabel('Time')
        ax.set_ylabel(var)
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    save_plot(fig, "data_visualization.png")
    
    # Correlation analysis
    correlation_summary = "Correlation analysis:\n"
    for country, df in list(country_data.items())[:2]:  # Show first 2 countries
        correlation_summary += f"\n{country} - Correlation matrix:\n"
        corr_matrix = df[variables].corr()
        correlation_summary += corr_matrix.round(3).to_string() + "\n"
        
        # Save correlation matrix
        save_dataframe(corr_matrix.round(3), f"correlation_matrix_{country}.csv")
    
    save_output(correlation_summary, "correlation_analysis.txt")
    
    # ===== DATA PREPROCESSING =====
    logging.info("\n" + "="*50)
    logging.info("DATA PREPROCESSING")
    logging.info("="*50)
    
    # 1. Handle missing values
    logging.info("Handling missing values...")
    processed_data = handle_missing_values(
        country_data, 
        policy=config['data']['missing']
    )
    
    # Check for missing values
    missing_summary = "Missing values after processing:\n"
    for country, df in processed_data.items():
        missing_count = df.isnull().sum().sum()
        missing_summary += f"{country}: {missing_count} missing values\n"
    
    logging.info(missing_summary)
    save_output(missing_summary, "missing_values_summary.txt")
    
    # 2. Create time splits
    logging.info("Creating time splits...")
    train_data, test_data, dropped_countries = create_time_split(
        processed_data,
        train_start=config['splits']['train_start'],
        test_cutoff=config['splits']['test_cutoff'],
        min_train_points=config['splits']['min_train_points']
    )
    
    split_summary = f"""Train/test split created:
  Training countries: {len(train_data)}
  Test countries: {len(test_data)}
  Dropped countries: {dropped_countries}

Split sizes:"""
    
    for country in list(train_data.keys())[:3]:  # First 3 countries
        train_size = len(train_data[country])
        test_size = len(test_data[country])
        split_summary += f"\n  {country}: {train_size} train, {test_size} test"
    
    logging.info(split_summary)
    save_output(split_summary, "train_test_split_summary.txt")
    
    # 3. Scale features
    logging.info("Scaling features...")
    scaled_train, scaled_test, scalers = scale_features(
        train_data=train_data,
        test_data=test_data,
        policy=config['data']['scale'],
        target_col=config['data']['target'],
        scale_target=config['data']['scale_target'],
        trimming=config['data'].get('trimming', None),
    )
    
    logging.info(f"Feature scaling completed using '{config['data']['scale']}' policy")
    
    # ===== FEATURE ENGINEERING =====
    logging.info("\n" + "="*50)
    logging.info("FEATURE ENGINEERING")
    logging.info("="*50)
    
    # 1. Create lagged features
    logging.info("Creating lagged features...")
    lagged_train = create_lagged_features(
        scaled_train, 
        lags=config['data']['lags']
    )
    
    lagged_test = create_lagged_features(
        scaled_test, 
        lags=config['data']['lags']
    )
    
    # Show example of lagged features
    sample_country = list(lagged_train.keys())[0]
    feature_summary = f"""Lagged features for {sample_country}:
Original columns: {list(scaled_train[sample_country].columns)}
With lags: {list(lagged_train[sample_country].columns)}
Data shape: {lagged_train[sample_country].shape}"""
    
    logging.info(feature_summary)
    save_output(feature_summary, "feature_engineering_summary.txt")
    
    # 2. Create forecast targets
    logging.info("Creating forecast targets...")
    target_train = create_forecast_targets(
        lagged_train,
        target_col=config['data']['target'],
        horizons=config['data']['horizons']
    )
    
    target_test = create_forecast_targets(
        lagged_test,
        target_col=config['data']['target'],
        horizons=config['data']['horizons']
    )
    
    # Show target structure
    target_cols = [f"{config['data']['target']}_h{h}" for h in config['data']['horizons']]
    target_summary = f"Target columns: {target_cols}\n"
    
    for country in list(target_train.keys())[:2]:
        target_summary += f"{country} training shape: {target_train[country].shape}\n"
        target_summary += f"{country} test shape: {target_test[country].shape}\n"
    
    logging.info(target_summary)
    save_output(target_summary, "target_creation_summary.txt")
    
    # Save sample of processed data
    sample_processed = target_train[sample_country][['TIME'] + target_cols].head()
    save_dataframe(sample_processed, f"processed_data_sample_{sample_country}.excsv")
    
    # ===== MODEL TRAINING =====
    logging.info("\n" + "="*50)
    logging.info("MODEL TRAINING")
    logging.info("="*50)
    
    def create_dummy_variables(data_list, time_col="TIME", add_country_dummies=False):
        """Create dummy variables for countries if needed"""
        num_of_countries = len(data_list)
        if not add_country_dummies:
            return data_list
        
        new_data_list = []
        for i, df in enumerate(data_list):
            df_copy = df.copy()
            for j in range(num_of_countries):
                dummy_col = f'dummy-{j}'
                if i != j:
                    df_copy[dummy_col] = 0
                else:
                    df_copy[dummy_col] = 1
            new_data_list.append(df_copy)
        
        return new_data_list
    
    # Prepare data for models
    train_data_list = []
    test_data_list = []
    
    for country, df in scaled_train.items():
        df_copy = df.copy()
        train_data_list.append(df_copy)
    
    for country, df in scaled_test.items():
        df_copy = df.copy()
        test_data_list.append(df_copy)
    
    logging.info(f"Prepared data for {len(train_data_list)} countries")
    
    # Ensure all data types are proper
    for i, df in enumerate(train_data_list):
        if 'TIME' in df.columns and df['TIME'].dtype == 'object':
            train_data_list[i]['TIME'] = pd.to_datetime(df['TIME'])
                
    for i, df in enumerate(test_data_list):
        if 'TIME' in df.columns and df['TIME'].dtype == 'object':
            test_data_list[i]['TIME'] = pd.to_datetime(df['TIME'])
    
    if config['ensemble_nn']['country_dummies'] and not config['ensemble_nn']['per_country']:
        global_train_data_list = create_dummy_variables(
            train_data_list,
            time_col="TIME",
            add_country_dummies=config['ensemble_nn']['country_dummies']
        )
        
        global_test_data_list = create_dummy_variables(
            test_data_list,
            time_col="TIME",
            add_country_dummies=config['ensemble_nn']['country_dummies']
        )
    else:
        global_train_data_list = train_data_list
        global_test_data_list = test_data_list
    
    # Train Factor Neural Network
    logging.info("\n" + "="*50)
    logging.info("Training Factor Neural Network")
    logging.info("="*50)
    
    n_countries = len(train_data_list)
    logging.info(f"Available countries: {n_countries}")
    
    country_names = [country for country in list(target_train.keys())]
    
    if config['ensemble_nn']['per_country']:
        fnn_models = {}
        for i, df in enumerate(train_data_list):
            logging.info(f"Training Factor NN for {i}...")
            
            try:
                ensemble_nn = EnsembleNNAPI(
                    data_list=[df],
                    target=config['data']['target'],
                    quantiles=config['data']['quantiles'],
                    forecast_horizons=config['data']['horizons'],
                    units_per_layer=config['ensemble_nn']['units_per_layer'],
                    lags=config['data']['lags'],
                    activation=config['ensemble_nn']['activation'],
                    device=config['ensemble_nn']['device'],
                    seed=config['seed'], 
                    transform=True,
                )
            
                ensemble_results = ensemble_nn.fit(
                    epochs=config['ensemble_nn']['epochs'],
                    learning_rate=config['ensemble_nn']['learning_rate'],
                    batch_size=config['ensemble_nn']['batch_size'],
                    validation_size=config['splits']['validation_size'],
                    patience=config['ensemble_nn']['patience'],
                    verbose=1,
                    optimizer=config['ensemble_nn']['optimizer'],
                    parallel_models=config['ensemble_nn']['parallel_models'],
                    l2=config['ensemble_nn']['l2_penalty'],
                    return_validation_loss=True,
                    return_train_loss=True,
                    shuffle=True
                )
                
                logging.info(f"✓ Factor NN for {i} trained successfully!")
                
                # Store model and results
                fnn_models[i] = {
                    'model': ensemble_nn,
                    'results': ensemble_results
                }
                
                # Plot training curves if available
                if 'train_losses' in ensemble_results and 'val_losses' in ensemble_results:
                    fig, ax = plt.subplots(figsize=(10, 6))
                    train_losses = ensemble_results['train_losses'][min(config['ensemble_nn']['patience']-1,5):]
                    val_losses = ensemble_results['val_losses'][min(config['ensemble_nn']['patience']-1,5):]
                    
                    ax.plot(train_losses, label='Training Loss', linewidth=2)
                    ax.plot(val_losses, label='Validation Loss', linewidth=2)
                    ax.set_xlabel('Epoch')
                    ax.set_ylabel('Loss')
                    ax.set_title(f'Factor NN Training Curves - Country {i}')
                    ax.legend()
                    ax.grid(True, alpha=0.3)
                    
                    save_plot(fig, f"fnn_training_curves_country_{i}.png")
                
            except Exception as e:
                logging.error(f"Factor NN training failed for country {i}: {e}")
                continue
        
        logging.info(f"Trained Factor NN models for {len(fnn_models)} countries.")
        fnn_training_successful = len(fnn_models) > 0
        ensemble_nn = None
        
    else:
        try:
            # Initialize Ensemble NN using high-level API
            ensemble_nn = EnsembleNNAPI(
                data_list=global_train_data_list,
                target=config['data']['target'],
                quantiles=config['data']['quantiles'],
                forecast_horizons=config['data']['horizons'],
                units_per_layer=config['ensemble_nn']['units_per_layer'],
                lags=config['data']['lags'],
                activation=config['ensemble_nn']['activation'],
                device=config['ensemble_nn']['device'],
                seed=config['seed'], 
                transform=True,
                prefit_AR=True,
                country_ids=country_names,
                time_col="TIME",
                verbose=1
            )
            
            training_summary = f"""✓ Ensemble NN initialized successfully!
  Input dimension: {ensemble_nn.input_dim}
  Target quantiles: {ensemble_nn.quantiles}
  Forecast horizons: {ensemble_nn.forecast_horizons}"""
            
            logging.info(training_summary)
            save_output(training_summary, "fnn_training_summary.txt")
            
            # Train with cross-validation
            logging.info("Starting training with cross-validation...")
            ensemble_results = ensemble_nn.fit(
                epochs=config['ensemble_nn']['epochs'],
                learning_rate=config['ensemble_nn']['learning_rate'],
                batch_size=config['ensemble_nn']['batch_size'],
                validation_size=config['splits']['validation_size'],
                patience=config['ensemble_nn']['patience'],
                verbose=1,
                optimizer=config['ensemble_nn']['optimizer'],
                parallel_models=config['ensemble_nn']['parallel_models'],
                l2=config['ensemble_nn']['l2_penalty'],
                return_validation_loss=True,
                return_train_loss=True,
                shuffle=True
            )
            
            final_summary = f"""✓ Ensemble NN training completed successfully!
  Results keys: {list(ensemble_results.keys())}
  Number of parameters: {ensemble_results.get('n_parameters', 'Unknown')}
  Final validation loss: {ensemble_results.get('final_val_loss', 'Unknown')}"""
            
            logging.info(final_summary)
            save_output(final_summary, "fnn_final_results.txt")
            
            # Plot training curves if available
            if 'train_losses' in ensemble_results and 'val_losses' in ensemble_results:
                fig, ax = plt.subplots(figsize=(10, 6))
                ax.plot(ensemble_results['train_losses'], label='Training Loss', linewidth=2)
                ax.plot(ensemble_results['val_losses'], label='Validation Loss', linewidth=2)
                ax.set_xlabel('Epoch')
                ax.set_ylabel('Loss')
                ax.set_title('Ensemble NN Training Curves')
                ax.legend()
                ax.grid(True, alpha=0.3)
                
                save_plot(fig, "ensemble_nn_training_curves.png")
            
            fnn_training_successful = True
            
        except Exception as e:
            logging.error(f"Factor NN training failed: {e}")
            logging.error("Traceback:")
            logging.error(traceback.format_exc())
            
            ensemble_nn = None
            fnn_training_successful = False
    
    # Train Linear Quantile Regression per country
    logging.info("\n" + "="*50)
    logging.info("Training Linear Quantile Regression")
    logging.info("="*50)
    
    lqr_models = {}
    
    for i, data in enumerate(train_data_list):
        try:
            # Initialize LQR using high-level API
            lqr_model = LQRModel(
                data_list=[data],
                target=config['data']['target'],
                quantiles=config['data']['quantiles'],
                forecast_horizons=config['data']['horizons'],
                lags=config['data']['lags'],
                alpha=1.0,
                fit_intercept=True,
                solver=config['lqr']['solver'],
                seed=config['seed']
            )
            
            # Cross-validate regularization parameter
            best_alpha = lqr_model.k_fold_validation(
                alphas=config['lqr']['alphas'],
                n_splits=10
            )
            
            logging.info(f"Best alpha for country {i}: {best_alpha}")
            
            # Fit final model
            lqr_coefficients = lqr_model.fit()
            
            lqr_models[i] = {
                'model': lqr_model,
                'coefficients': lqr_coefficients
            }
            
        except Exception as e:
            logging.error(f"LQR training failed for country {i}: {e}")
            continue
    
    # Train LQR AR models (autoregressive only)
    logging.info("\nTraining LQR AR models...")
    lqr_ar_models = {}
    
    for i, data in enumerate(train_data_list):
        try:
            # Only use the target column and the time column
            data_temp = data[['TIME', config['data']['target']]].copy()
            
            lqr_model = LQRModel(
                data_list=[data_temp],
                target=config['data']['target'],
                quantiles=config['data']['quantiles'],
                forecast_horizons=config['data']['horizons'],
                lags=config['data']['lags'],
                alpha=0.0,
                fit_intercept=True,
                solver=config['lqr']['solver'],
                seed=config['seed']
            )
            
            # Fit final model
            lqr_coefficients = lqr_model.fit()
            
            lqr_ar_models[i] = {
                'model': lqr_model,
                'coefficients': lqr_coefficients
            }
            
        except Exception as e:
            logging.error(f"LQR AR training failed for country {i}: {e}")
            continue
    
    # Status summary
    status_summary = f"""TRAINING STATUS SUMMARY:
✓ Data preprocessing: SUCCESSFUL
✓ Feature engineering: SUCCESSFUL 
✓ Dataset creation: SUCCESSFUL
✓ Data type handling: SUCCESSFUL
{'✓' if fnn_training_successful else '🚨'} Factor NN training: {'SUCCESSFUL' if fnn_training_successful else 'FAILED'}
✓ LQR training: SUCCESSFUL ({len(lqr_models)} models)
✓ LQR AR training: SUCCESSFUL ({len(lqr_ar_models)} models)"""
    
    logging.info(status_summary)
    save_output(status_summary, "training_status_summary.txt")
    
    # ===== MODEL PREDICTIONS =====
    logging.info("\n" + "="*50)
    logging.info("GENERATING PREDICTIONS")
    logging.info("="*50)
    
    # Generate predictions for each model
    lqr_per_country_predictions, lqr_per_country_targets = {}, {}
    country_names = [country for country in list(target_train.keys())]
    
    for idx, test_data in enumerate(test_data_list):
        if idx in lqr_models:
            model = lqr_models[idx]['model']
            predictions, targets = model.predict([test_data])
            lqr_per_country_predictions[country_names[idx]] = predictions
            lqr_per_country_targets[country_names[idx]] = targets
    
    lqr_ar_per_country_predictions, lqr_ar_per_country_targets = {}, {}
    
    for idx, test_data in enumerate(test_data_list):
        if idx in lqr_ar_models:
            model = lqr_ar_models[idx]['model']
            predictions, targets = model.predict([test_data[['TIME', config['data']['target']]]])
            lqr_ar_per_country_predictions[country_names[idx]] = predictions
            lqr_ar_per_country_targets[country_names[idx]] = targets
    
    fnn_per_country_predictions, fnn_per_country_targets = {}, {}
    
    for idx, test_data in enumerate(global_test_data_list):
        if config['ensemble_nn']['per_country']:
            if idx in fnn_models:
                model = fnn_models[idx]['model']
                predictions, targets = model.predict_per_country(test_data, 0)
                fnn_per_country_predictions[country_names[idx]] = predictions
                fnn_per_country_targets[country_names[idx]] = targets
        else:
            if ensemble_nn:
                predictions, targets = ensemble_nn.predict_per_country(test_data, country_names[idx])
                fnn_per_country_predictions[country_names[idx]] = predictions
                fnn_per_country_targets[country_names[idx]] = targets
    
    prediction_summary = f"""Prediction Generation Summary:
FNN predictions: {len(fnn_per_country_predictions)} countries
LQR predictions: {len(lqr_per_country_predictions)} countries  
LQR AR predictions: {len(lqr_ar_per_country_predictions)} countries"""
    
    logging.info(prediction_summary)
    save_output(prediction_summary, "prediction_summary.txt")
    
    # ===== FORECAST VISUALIZATION =====
    logging.info("\n" + "="*50)
    logging.info("FORECAST VISUALIZATION")
    logging.info("="*50)
    
    countries = list(scaled_test.keys())
    n_countries_to_plot = len(countries)
    div_2 = (n_countries_to_plot + 1) // 2
    
    for h, horizon in enumerate(config['data']['horizons']):
        fig, axes = plt.subplots(div_2, 2, figsize=(15, 4*div_2))
        if div_2 == 1:
            axes = axes.reshape(1, -1)
        axes = axes.flatten()
        
        logging.info(f"Creating forecast plots for horizon {horizon}...")
        
        for i, country in enumerate(countries[:n_countries_to_plot]):
            if i >= len(axes):
                break
                
            ax = axes[i]
            
            # Check if we have predictions for this country
            if (country in fnn_per_country_targets and 
                country in lqr_per_country_targets and 
                country in lqr_ar_per_country_targets):
                
                # Extract actual values for this country and horizon
                if fnn_per_country_targets[country].ndim == 2:
                    actual = fnn_per_country_targets[country][:, h]
                else:
                    actual = fnn_per_country_targets[country]
                
                # Extract predictions for this country and horizon
                if (country in fnn_per_country_predictions and 
                    fnn_per_country_predictions[country] is not None):
                    fnn_pred_this_horizon = fnn_per_country_predictions[country][:, h]
                else:
                    fnn_pred_this_horizon = np.full((len(actual), len(config['data']['quantiles'])), np.nan)
                
                if (country in lqr_per_country_predictions and 
                    lqr_per_country_predictions[country] is not None):
                    lqr_pred_this_horizon = lqr_per_country_predictions[country][:, h]
                else:
                    lqr_pred_this_horizon = np.full((len(actual), len(config['data']['quantiles'])), np.nan)
                
                if (country in lqr_ar_per_country_predictions and 
                    lqr_ar_per_country_predictions[country] is not None):
                    lqr_ar_pred_this_horizon = lqr_ar_per_country_predictions[country][:, h]
                else:
                    lqr_ar_pred_this_horizon = np.full((len(actual), len(config['data']['quantiles'])), np.nan)
                
                # Extract quantiles (assuming first is lower, second is upper)
                try:
                    fnn_pred_q05 = fnn_pred_this_horizon[:, 0] if fnn_pred_this_horizon.shape[1] > 0 else np.full(len(actual), np.nan)
                    fnn_pred_q95 = fnn_pred_this_horizon[:, 1] if fnn_pred_this_horizon.shape[1] > 1 else np.full(len(actual), np.nan)
                    lqr_pred_q05 = lqr_pred_this_horizon[:, 0] if lqr_pred_this_horizon.shape[1] > 0 else np.full(len(actual), np.nan)
                    lqr_pred_q95 = lqr_pred_this_horizon[:, 1] if lqr_pred_this_horizon.shape[1] > 1 else np.full(len(actual), np.nan)
                    lqr_ar_pred_q05 = lqr_ar_pred_this_horizon[:, 0] if lqr_ar_pred_this_horizon.shape[1] > 0 else np.full(len(actual), np.nan)
                    lqr_ar_pred_q95 = lqr_ar_pred_this_horizon[:, 1] if lqr_ar_pred_this_horizon.shape[1] > 1 else np.full(len(actual), np.nan)
                except (IndexError, AttributeError):
                    fnn_pred_q05 = fnn_pred_q95 = np.full(len(actual), np.nan)
                    lqr_pred_q05 = lqr_pred_q95 = np.full(len(actual), np.nan)
                    lqr_ar_pred_q05 = lqr_ar_pred_q95 = np.full(len(actual), np.nan)
                
                # Time index for plotting
                time_idx = range(len(actual))
                
                # Plot actual values
                ax.plot(time_idx, actual, 'k-', linewidth=2, label='Actual', alpha=0.8)
                
                # Plot predictions with uncertainty bands
                if not np.all(np.isnan(fnn_pred_q05)):
                    ax.fill_between(time_idx, fnn_pred_q05, fnn_pred_q95, 
                                   alpha=0.3, color='blue', label='FNN 90% CI')
                    fnn_median = (fnn_pred_q05 + fnn_pred_q95) / 2
                    ax.plot(time_idx, fnn_median, 'b--', 
                           linewidth=1.5, label='FNN Median', alpha=0.8)
                
                if not np.all(np.isnan(lqr_pred_q05)):
                    ax.fill_between(time_idx, lqr_pred_q05, lqr_pred_q95, 
                                   alpha=0.2, color='red', label='LQR 90% CI')
                    lqr_median = (lqr_pred_q05 + lqr_pred_q95) / 2
                    ax.plot(time_idx, lqr_median, 'r:', 
                           linewidth=1.5, label='LQR Median', alpha=0.8)
                
                if not np.all(np.isnan(lqr_ar_pred_q05)):
                    ax.fill_between(time_idx, lqr_ar_pred_q05, lqr_ar_pred_q95, 
                                   alpha=0.2, color='green', label='LQR AR 90% CI')
                    lqr_ar_median = (lqr_ar_pred_q05 + lqr_ar_pred_q95) / 2
                    ax.plot(time_idx, lqr_ar_median, 'g-.', 
                           linewidth=1.5, label='LQR AR Median', alpha=0.8)
                
                ax.set_title(f'{country} - Horizon {horizon}')
                ax.set_xlabel('Time Period')
                ax.set_ylabel(config['data']['target'])
                ax.legend(fontsize=8)
                ax.grid(True, alpha=0.3)
            else:
                ax.text(0.5, 0.5, f'No predictions\navailable for\n{country}', 
                       transform=ax.transAxes, ha='center', va='center')
                ax.set_title(f'{country} - Horizon {horizon}')
        
        # Hide unused subplots
        for i in range(n_countries_to_plot, len(axes)):
            axes[i].set_visible(False)
        
        plt.suptitle(f'Forecast Comparison - {horizon}-Period Ahead', fontsize=16)
        plt.tight_layout()
        save_plot(fig, f"forecast_comparison_horizon_{horizon}.png")
    
    # ===== STATISTICAL SIGNIFICANCE TESTING =====
    logging.info("\n" + "="*50)
    logging.info("STATISTICAL SIGNIFICANCE TESTING")
    logging.info("="*50)
    
    def compare_models_dm_test(model1_predictions, model2_predictions, targets, 
                              model1_name="Model1", model2_name="Model2", countries=None):
        """Perform Diebold-Mariano test for forecast accuracy comparison"""
        
        if countries is None:
            countries = list(model1_predictions.keys())
        
        dm_results_per_country = {}
        
        # Per-Country Diebold-Mariano Tests
        for country in countries:
            if (model1_predictions.get(country) is not None and 
                model2_predictions.get(country) is not None and
                targets.get(country) is not None):
                
                dm_results_per_country[country] = {}
                country_summary = f"\n{country} - Diebold-Mariano Tests ({model1_name} vs {model2_name}):\n"
                
                for h, horizon in enumerate(config['data']['horizons']):
                    dm_results_per_country[country][horizon] = {}
                    
                    for q, quantile in enumerate(config['data']['quantiles']):
                        # Get targets for this country and horizon
                        if targets[country].ndim == 2:
                            actual = targets[country][:, h]
                        else:
                            actual = targets[country]
                        
                        # Get predictions for this country, horizon, and quantile
                        if (model1_predictions[country].ndim >= 2 and 
                            model2_predictions[country].ndim >= 2):
                            model1_pred = model1_predictions[country][:, h][:, q]
                            model2_pred = model2_predictions[country][:, h][:, q]
                            
                            if len(actual) > 5:  # Need minimum samples for meaningful test
                                # Calculate quantile losses
                                model1_qloss = pinball_loss(actual, model1_pred, quantile)
                                model2_qloss = pinball_loss(actual, model2_pred, quantile)
                                
                                # Diebold-Mariano test statistic
                                loss_diff = model1_qloss - model2_qloss
                                
                                # Calculate test statistic
                                mean_diff = np.mean(loss_diff)
                                var_diff = np.var(loss_diff, ddof=1)
                                
                                if var_diff > 0:
                                    dm_stat = mean_diff / np.sqrt(var_diff / len(loss_diff))
                                    p_value = 2 * (1 - stats.norm.cdf(abs(dm_stat)))
                                    
                                    dm_results_per_country[country][horizon][quantile] = {
                                        'dm_statistic': dm_stat,
                                        'p_value': p_value,
                                        'significant': p_value < 0.05,
                                        'better_model': model2_name if dm_stat > 0 else model1_name
                                    }
                                    
                                    significance = "***" if p_value < 0.01 else "**" if p_value < 0.05 else "*" if p_value < 0.1 else ""
                                    
                                    country_summary += f"  H{horizon} Q{quantile:.2f}: DM={dm_stat:.3f}, p={p_value:.3f}{significance} -> {dm_results_per_country[country][horizon][quantile]['better_model']}\n"
                                else:
                                    dm_results_per_country[country][horizon][quantile] = {
                                        'dm_statistic': 0,
                                        'p_value': 1.0,
                                        'significant': False,
                                        'better_model': 'Tie'
                                    }
                                    country_summary += f"  H{horizon} Q{quantile:.2f}: No variance in loss differences\n"
                            else:
                                dm_results_per_country[country][horizon][quantile] = {
                                    'dm_statistic': np.nan,
                                    'p_value': np.nan,
                                    'significant': False,
                                    'better_model': 'Insufficient data'
                                }
                                country_summary += f"  H{horizon} Q{quantile:.2f}: Insufficient data for testing\n"
                
                logging.info(country_summary)
                save_output(country_summary, f"dm_test_{country}_{model1_name}_vs_{model2_name}.txt")
        
        return dm_results_per_country
    
    # Perform DM tests for FNN vs LQR
    dm_results_fnn_lqr = compare_models_dm_test(
        fnn_per_country_predictions, 
        lqr_per_country_predictions, 
        fnn_per_country_targets,
        model1_name="FNN",
        model2_name="LQR",
        countries=countries
    )
    
    # ===== PER-COUNTRY QUANTILE LOSS ANALYSIS =====
    logging.info("\n" + "="*50)
    logging.info("PER-COUNTRY QUANTILE LOSS ANALYSIS")
    logging.info("="*50)
    
    def compute_per_country_metrics(model1_predictions, model2_predictions, targets,
                                   model1_name="Model1", model2_name="Model2", countries=None):
        """Calculate Quantile Loss for each country, quantile, and horizon"""
        
        if countries is None:
            countries = list(model1_predictions.keys())
        
        per_country_metrics = {}
        
        for country in countries:
            if (model1_predictions.get(country) is not None and 
                model2_predictions.get(country) is not None and
                targets.get(country) is not None):
                
                per_country_metrics[country] = {}
                
                for h, horizon in enumerate(config['data']['horizons']):
                    per_country_metrics[country][horizon] = {}
                    
                    # Get targets for this country and horizon
                    if targets[country].ndim == 2:
                        actual = targets[country][:, h]
                    else:
                        actual = targets[country]
                    
                    for q, quantile in enumerate(config['data']['quantiles']):
                        per_country_metrics[country][horizon][quantile] = {}
                        
                        # Get predictions for this country, horizon, and quantile
                        if (model1_predictions[country].ndim >= 2 and 
                            model2_predictions[country].ndim >= 2):
                            model1_pred = model1_predictions[country][:, h][:, q]
                            model2_pred = model2_predictions[country][:, h][:, q]
                            
                            # Calculate Quantile Loss for each model
                            for model_name, pred in [(model1_name, model1_pred), (model2_name, model2_pred)]:
                                q_loss = np.mean(pinball_loss(actual, pred, quantile))
                                per_country_metrics[country][horizon][quantile][model_name] = q_loss
        
        return per_country_metrics
    
    # Calculate metrics for FNN vs LQR
    per_country_metrics = compute_per_country_metrics(
        fnn_per_country_predictions,
        lqr_per_country_predictions,
        fnn_per_country_targets,
        model1_name="FNN",
        model2_name="LQR", 
        countries=countries
    )
    
    # Create metrics comparison table
    def create_metrics_comparison_table(per_country_metrics, countries, 
                                       model1_name="Model1", model2_name="Model2"):
        """Create comparison tables and compute aggregate metrics"""
        
        # Create per-country metrics table
        country_metrics_data = []
        
        for country in countries:
            if country in per_country_metrics:
                for horizon in config['data']['horizons']:
                    for quantile in config['data']['quantiles']:
                        if (horizon in per_country_metrics[country] and 
                            quantile in per_country_metrics[country][horizon]):
                            model1_qloss = per_country_metrics[country][horizon][quantile].get(model1_name, np.nan)
                            model2_qloss = per_country_metrics[country][horizon][quantile].get(model2_name, np.nan)
                            
                            if not np.isnan(model1_qloss) and not np.isnan(model2_qloss) and model2_qloss != 0:
                                improvement = ((model2_qloss - model1_qloss) / model2_qloss) * 100
                                better_model = model1_name if model1_qloss < model2_qloss else model2_name
                            else:
                                improvement = np.nan
                                better_model = "N/A"
                            
                            country_metrics_data.append({
                                'Country': country,
                                'Horizon': horizon,
                                'Quantile': quantile,
                                f'{model1_name}_QLoss': model1_qloss,
                                f'{model2_name}_QLoss': model2_qloss,
                                'Improvement_%': improvement,
                                'Better_Model': better_model
                            })
        
        country_metrics_df = pd.DataFrame(country_metrics_data)
        save_dataframe(country_metrics_df.round(4), f"per_country_metrics_{model1_name}_vs_{model2_name}.csv")
        
        # Create aggregate summary
        aggregate_metrics_data = []
        
        for horizon in config['data']['horizons']:
            for quantile in config['data']['quantiles']:
                # Aggregate across all countries
                all_model1_qloss = []
                all_model2_qloss = []
                
                for country in countries:
                    if (country in per_country_metrics and 
                        horizon in per_country_metrics[country] and
                        quantile in per_country_metrics[country][horizon]):
                        model1_val = per_country_metrics[country][horizon][quantile].get(model1_name, np.nan)
                        model2_val = per_country_metrics[country][horizon][quantile].get(model2_name, np.nan)
                        
                        if not np.isnan(model1_val) and not np.isnan(model2_val):
                            all_model1_qloss.append(model1_val)
                            all_model2_qloss.append(model2_val)
                
                if len(all_model1_qloss) > 0:
                    avg_model1_qloss = np.mean(all_model1_qloss)
                    avg_model2_qloss = np.mean(all_model2_qloss)
                    
                    if avg_model2_qloss != 0:
                        improvement = ((avg_model2_qloss - avg_model1_qloss) / avg_model2_qloss) * 100
                        better_model = model1_name if avg_model1_qloss < avg_model2_qloss else model2_name
                    else:
                        improvement = np.nan
                        better_model = "N/A"
                    
                    aggregate_metrics_data.append({
                        'Horizon': horizon,
                        'Quantile': quantile,
                        f'{model1_name}_QLoss_Avg': avg_model1_qloss,
                        f'{model2_name}_QLoss_Avg': avg_model2_qloss,
                        'Improvement_%': improvement,
                        'Better_Model': better_model
                    })
        
        aggregate_df = pd.DataFrame(aggregate_metrics_data)
        save_dataframe(aggregate_df.round(4), f"aggregate_metrics_{model1_name}_vs_{model2_name}.csv")
        
        return country_metrics_df, aggregate_df
    
    # Generate tables for FNN vs LQR comparison
    country_metrics_df, aggregate_df = create_metrics_comparison_table(
        per_country_metrics, countries, 
        model1_name="FNN", model2_name="LQR"
    )
    
    # ===== FINAL SUMMARY =====
    logging.info("\n" + "="*60)
    logging.info("FINAL QUANTILE LOSS EVALUATION SUMMARY")
    logging.info("="*60)
    
    def generate_final_summary(country_metrics_df, per_country_metrics, countries,
                              model1_name="Model1", model2_name="Model2"):
        """Generate final summary and recommendations"""
        
        # Calculate win rates for Quantile Loss
        qloss_win_summary = {
            f'{model1_name}_wins': 0,
            f'{model2_name}_wins': 0,
            'total_comparisons': len(country_metrics_df)
        }
        
        for _, row in country_metrics_df.iterrows():
            if not pd.isna(row[f'{model1_name}_QLoss']) and not pd.isna(row[f'{model2_name}_QLoss']):
                if row[f'{model1_name}_QLoss'] < row[f'{model2_name}_QLoss']:
                    qloss_win_summary[f'{model1_name}_wins'] += 1
                else:
                    qloss_win_summary[f'{model2_name}_wins'] += 1
        
        final_summary = f"""FINAL QUANTILE LOSS EVALUATION SUMMARY ({model1_name} vs {model2_name})
{'='*70}

Quantile Loss Win Rate Summary:
{'-'*50}"""
        
        model1_wins = qloss_win_summary[f'{model1_name}_wins']
        total = qloss_win_summary['total_comparisons']
        if total > 0:
            model1_rate = (model1_wins / total) * 100
            final_summary += f"\nQuantile Loss: {model1_name} wins {model1_wins}/{total} ({model1_rate:.1f}%)"
        
        # Country-by-Country Summary
        final_summary += f"\n\nCountry-by-Country Performance Summary:\n{'-'*60}"
        for country in countries:
            if country in per_country_metrics:
                country_data = country_metrics_df[country_metrics_df['Country'] == country]
                if len(country_data) > 0:
                    country_model1_wins = sum(1 for _, row in country_data.iterrows() 
                                            if row['Better_Model'] == model1_name)
                    country_total = len(country_data)
                    country_model1_rate = (country_model1_wins / country_total) * 100 if country_total > 0 else 0
                    
                    avg_improvement = country_data['Improvement_%'].mean()
                    
                    final_summary += f"\n{country}: {model1_name} wins {country_model1_wins}/{country_total} ({country_model1_rate:.1f}%) | "
                    final_summary += f"Avg improvement: {avg_improvement:+.2f}%"
        
        # Overall recommendation
        if total > 0:
            if model1_rate > 60:
                recommendation = f"{model1_name} shows superior quantile forecasting performance"
            elif model1_rate < 40:
                recommendation = f"{model2_name} shows superior quantile forecasting performance"
            else:
                recommendation = f"Both {model1_name} and {model2_name} show comparable quantile forecasting performance"
        else:
            recommendation = "Insufficient data for meaningful comparison"
        
        final_summary += f"\n\nOverall Recommendation: {recommendation}"
        
        # Performance by Quantile and Horizon
        final_summary += f"\n\nPerformance by Quantile and Horizon:\n{'-'*50}"
        for horizon in config['data']['horizons']:
            final_summary += f"\n\nHorizon {horizon}:"
            for quantile in config['data']['quantiles']:
                horizon_quantile_data = aggregate_df[
                    (aggregate_df['Horizon'] == horizon) & 
                    (aggregate_df['Quantile'] == quantile)
                ]
                if not horizon_quantile_data.empty:
                    row = horizon_quantile_data.iloc[0]
                    final_summary += f"\n  Quantile {quantile:.2f}: {row['Better_Model']} wins | "
                    final_summary += f"Improvement: {row['Improvement_%']:+.2f}%"
        
        final_summary += f"""

Key Insights:
• Quantile Loss is the most relevant metric for quantile forecasting evaluation
• Per-country analysis reveals model performance heterogeneity across countries
• Different quantiles may have varying predictability across forecast horizons
• Country-specific characteristics may favor different modeling approaches

✓ Complete per-country quantile loss evaluation finished ({model1_name} vs {model2_name})!"""
        
        logging.info(final_summary)
        save_output(final_summary, f"final_summary_{model1_name}_vs_{model2_name}.txt")
        
        return final_summary
    
    # Generate summary for FNN vs LQR
    final_summary = generate_final_summary(
        country_metrics_df, per_country_metrics, countries,
        model1_name="FNN", model2_name="LQR"
    )
    
    # ===== COMPLETION =====
    completion_message = f"""
ANALYSIS COMPLETED SUCCESSFULLY!
{'-'*50}

All outputs have been saved to: {output_dir.absolute()}

Generated files:
• Plots: {len(list(plots_dir.glob('*.png')))} visualization files
• Data: {len(list(data_dir.glob('*.csv')))} CSV files  
• Logs: {len(list(logs_dir.glob('*.txt')))} text output files
• Analysis log: {log_file}

Total runtime: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
    
    logging.info(completion_message)
    save_output(completion_message, "completion_summary.txt")
    
    print(f"\n✅ Analysis complete! Check the '{output_dir}' directory for all outputs.")

if __name__ == "__main__":
    main()


seed: 123

data:
  path: processed_per_country
  target: prc_hicp_manr_CP00
  required_columns: []
  lags: []
  horizons: [1]
  quantiles: [0.1]
  missing: forward_fill_then_mean   # or interpolate_linear | drop
  scale: per_country                # or global | none
  scale_target: false
  frequency: infer                  # assert equal spacing
  trimming: true

splits:
  validation_size: 0.3             # fraction of each country's data for validation
  train_start: "1997-01-01"
  test_cutoff: "2017-12-01"         # last date in training; all later dates are test
  min_train_points: 12              # enforce minimum per country

ensemble_nn:
  units_per_layer: [32,32,32,32,32]
  activation: relu
  optimizer: adam
  learning_rate: 5.0e-5
  epochs: 1500
  batch_size: 32
  patience: 50
  parallel_models: 10
  per_country: false
  device: auto   
  country_dummies: false                   # cuda if available
  l2_penalty: 0.0001 

lqr:
  alphas: [ 7.5, 10, 15, 20, 25, 30, 35]
  solver: huberized
  rff_features: 1000
  rolling: false

output:
  save_predictions: true
  save_factors: true
  save_models: true
