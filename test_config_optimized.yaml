# ============================
# OPTIMIZED Rolling Window Forecast Config
# ============================

seed: 123

data:
  path: processed_per_country          # folder containing one file per country (CSV/Parquet)
  time_col: TIME
  country_col: COUNTRY                 # optional; if missing we infer from filename
  target: prc_hicp_manr_CP00
  lags: []               # if empty, defaults to 1..window_size
  horizons: [1]
  quantiles: [0.1]
  missing: forward_fill_then_mean      # forward fill, then mean imputation

rolling_window:
  size: 215                             # training window size (in periods)
  step: 1                              # how many periods to advance the window
  start: auto                          # 'auto' => use splits.test_cutoff
  end: auto                            # 'auto' => last available date

splits:
  test_cutoff: "2017-12-01"            # if rolling_window.start == auto
  min_train_points: 24                 # minimum obs required to train

runtime:
  allow_reload: true                  # load existing model if present
  retrain_if_exists: false             # force retrain if model already exists
  num_workers: 15                       # hint for how many workers will be working
  batch_size: 100                      # LARGE batches to reduce coordination overhead
  max_ram_gb: 30.0                      # or specify float
  safety_ram_fraction: 0.8             # fraction of total RAM usable
  mem_probe_fudge_mb: 200              # extra margin on top of probe
  retries: 1                           # automatic retries on failure
  thread_pinning: false                # ALLOW multi-threading for inherently parallel ops
  progress_refresh_sec: 30             # REDUCE progress check frequency
  save_models: false                   # DISABLE model saving for speed

io:
  output_root: outputs
  models_dir: models
  forecasts_dir: forecasts
  progress_parquet: progress/progress.parquet
  errors_parquet: progress/errors.parquet
  logs_dir: logs

models:
  - type: ar-qr
    enabled: true
    params:
      solver: huberized
      alphas: [0.0]                    # REDUCE alpha search space
      use_cv: false                    # DISABLE CV for speed
      cv_splits: 3                     # REDUCE CV splits if enabled

  - type: lqr
    enabled: true
    params:
      solver: huberized
      alphas: [15, 25, 35]            # REDUCE alpha search space significantly
      use_cv: false                    # DISABLE CV for speed
      cv_splits: 3                     # REDUCE CV splits if enabled

  - type: nn
    enabled: false
    per_country: false  # true = train separate model for each country, false = train global model on all countries
    versions:
      - name: v1
        params:
          units_per_layer: [32, 32]     # REDUCE network complexity
          activation: relu
          optimizer: adam
          learning_rate: 1.0e-3         # INCREASE learning rate
          epochs: 500                   # REDUCE epochs significantly
          batch_size: 64
          patience: 20                  # REDUCE patience
          parallel_models: 1            # Keep at 1 for single-core
          device: cpu
          l2_penalty: 1.0e-4
          validation_size: 0.2          # REDUCE validation size
